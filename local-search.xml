<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>二进制部署k8s的master节点</title>
    <link href="/2020/09/04/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E7%9A%84master%E8%8A%82%E7%82%B9/"/>
    <url>/2020/09/04/%E4%BA%8C%E8%BF%9B%E5%88%B6%E9%83%A8%E7%BD%B2k8s%E7%9A%84master%E8%8A%82%E7%82%B9/</url>
    
    <content type="html"><![CDATA[<h3 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h3><p>Docker是容器引擎。</p><p>1、docker实现了NameSpace资源隔离：</p><ul><li>PID-进程编号</li><li>NET-网络设备、网络协议栈、端口</li><li>IPC-信号量、消息队列、共享内存</li><li>MOUNT-文件系统、挂载点</li><li>UTS-主机名和主机域</li><li>USER-操作进程的用户名和用户组</li></ul><p>2、docker引擎</p><p>docker引擎分为两个版本：企业版（EE）和社区版（CE）。</p><p>3、dockerFile</p><ul><li>USER/WORKDIR</li><li>ADD/EXPOSE</li><li>RUN/ENV</li><li>CMD/ENTRYPOINT</li></ul><p>4、docker网络模型</p><ul><li>NET（默认）</li><li>NONE（不需要提供网络服务）</li><li>HOST（宿主机网络）</li><li>联合网络（容器共享网络）</li></ul><h3 id="k8s入门"><a href="#k8s入门" class="headerlink" title="k8s入门"></a>k8s入门</h3><p>1、基本概念</p><ul><li><p>Pod/Pod控制器</p><p>Pod</p><blockquote><p>Pod是k8s是能够被运行的最小单元。一个pod里面运行多个容器，他们共享<code>UTS+NET+IPC</code>名称空间。    一个pod运行多个容器，又叫：<code>边车模式</code>（SideCar）</p></blockquote></li><li><p>Pod控制器</p><blockquote><p>pod控制器是pod启动的一种模板，用来保证k8s里启动的pod始终按照人们的预期运行（副本数、生命周期、健康状态检查……）</p></blockquote></li></ul><p>K8S内提供的众多Pod控制器，常用的有一下几种：</p><blockquote><blockquote><p>Depoloyment<br>DaemonSet<br>ReplicaSet<br>StatefulSet<br>Job<br>Cronjob</p></blockquote></blockquote><p>2）Name/Namespace</p><ul><li>Name</li></ul><blockquote><p>由于k8s内部，使用“资源”来定义每一种逻辑（功能）故每种“资源”都应该有自己的“”名称。</p></blockquote><ul><li>Namespace</li></ul><blockquote><p>随着项目增多、人员增加、集群规模扩大，需要一种能够隔离k8s内各种“资源”的方法，这就是名称空间。<br>名称空间可以理解为k8s内部的虚拟集群组。<br>k8s里默认的名称空间有：<code>default</code>、<code>kube-system</code>、<code>kube-piblic</code>。<br>查询k8s里特定的“资源”要带上相应的名称空间。</p></blockquote><p>3）Label/Label选择器</p><ul><li>Label</li></ul><blockquote><p>标签是k8s特色的管理方式，便于分类管理资源对象。<br>一个标签可以对应多个资源，一个资源也可以对应多个标签，他们是多对多的关系。<br>标签的组成：<code>key=value</code><br>与标签类似的，还有一种“注解”（annotations）</p></blockquote><ul><li>Label选择器</li></ul><blockquote><p>给资源打上标签后，可以使用标签选择器过滤掉指定的标签。<br>标签选择器目前有两个：基于<code>等值关系</code>（等于、不等于）和基于<code>集合关系</code>（属于、不属于、存在）。<br>许多资源支持内嵌标签选择器字段<br>matchLabels<br>matchExpressions</p></blockquote><p>4）Service/Ingress</p><ul><li>Service</li></ul><blockquote><p>在k8s里面，虽然每个pod都会被分配一个单独的IP地址，但这个IP地址会随着pod的销毁而消失。<br>Service（服务）就是解决这个问题的核心概念。<br>一个Service可以看作是一组提供相同服务的pod的对外访问接口<br>Service的作用哪些pod是通过标签选择器来定义的</p></blockquote><ul><li>Ingress</li></ul><blockquote><p>Ingress是k8s集群里工作在OSI网络参考模型下，第七层的应该，对外暴露的接口。<br>Service只能进行L4流量调度，表现形式<code>ip+port</code>。<br>Ingress则可以调度不同业务域，不同URL访问路径的业务流量。</p></blockquote><p>2、核心组件/附件</p><p>1）核心组件</p><blockquote><p>配置存储中心——》etcd服务</p><p>主控（master）节点：</p><blockquote><p>kube-apiserver服务</p><p>kube-contrlolre-manager服务</p><p>kube-scheduller服务</p></blockquote><p>运算（node）节点：</p><blockquote><p>kube-lubelet服务</p><p>kuble-proxy服务<br>    流量调度模式：<br>        Userspace（废弃）<br>        Iptables（目前常用，濒临废弃）<br>        Ipvs（推荐）</p></blockquote><p>CLI客户端：</p><pre><code class="hljs shell">kubectl</code></pre></blockquote><p>2）核心附件</p><blockquote><p>CNI网络插件——》flannel/calico</p><p>服务发现插件——》coredns</p><p>服务暴露插件——》traefik</p><p>GUI管理插件——》Dashboard</p></blockquote><p>3、部署</p><p>1）部署规划</p><p>​    生产部署规划</p><p>​        多master</p><blockquote><blockquote><p>master节点建议3台（16G内存）<br>etcd最少必须3台或以上（必须奇数台，解决选举问题）<br>worker节点越多越好，可根据实际业务情况（64G内存）</p></blockquote></blockquote><p>​    实验部署规划</p><blockquote><p>三个节点<br>2G内存<br>2核CPU</p></blockquote><p>2）部署方式</p><blockquote><p>minikube<br>kubeadm<br>二进制</p></blockquote><p><strong>3）二进制部署k8s</strong></p><p>Ⅰ：环境</p><blockquote><p>部署单master节点：<br>master：<br>    主机：vm001     192.168.220.128<br>worker：<br>    node1：vm002      192.168.220.129<br>    node2：vm003      192.168.220.130<br>k8s版本：1.16<br>系统版本：CentOS Linux release 7.8.2003 (Core)</p></blockquote><p>Ⅱ：初始化</p><ol><li><p>关闭防火墙（所以节点都执行）</p><pre><code class="hljs shell">systemctl stop firewalldsystemctl disable firewalld</code></pre></li><li><p>关闭SElinux</p><pre><code class="hljs shell">setenforce 0（临时关闭）vim /etc/selinux/config（永久关闭）SELINUX=disabled</code></pre></li><li><p>配置主机名（所以节点都执行）</p><pre><code class="hljs shell">hostnamectl set-hostname 主机名</code></pre></li><li><p>配置名称解析（所以节点都执行）</p><pre><code class="hljs shell">vim /etc/hosts</code></pre><blockquote><p>192.168.220.128 vm001<br>192.168.220.129 vm002<br>192.168.220.130 vm003<br>192.168.220.131 vm004（后期升级多master节点备用）</p></blockquote></li><li><p>配置时间同步</p><p>选择一个节点作为服务端，剩下的为客户端。<br>vm001为时间服务器的服务端，其他的为时间服务器客户端。</p><ul><li><p>配置vm001</p><pre><code class="hljs shell">yum install chrony -yvim /etc/chrony.config（修改以下三项配置）</code></pre><blockquote><p>server 127.127.1.0 iburst</p><pre><code>    allow 192.168.220.0/16    local stratum 10</code></pre></blockquote></li><li><p>配置客户端（vm002、vm003）</p><pre><code class="hljs shell">vim /etc/chrony.config</code></pre><blockquote><p>server 192.168.220.128 iburst（#指定vm001的IP即可，其他配置不动）</p></blockquote><pre><code class="hljs shell">systemctl start chronydsystemctl enable chronyd</code></pre><p>客户端检查时间是否同步（主要看是否为^<em>，^</em>代表时间同步）：</p><pre><code class="hljs shell">chronyc sources</code></pre><blockquote><p>210 Number of sources = 1</p><p>MS Name/IP address         Stratum Poll Reach LastRx Last sample               </p><p>^* vm001                         0   7     0     -     +0ns[   +0ns] +/-    0ns</p></blockquote><pre><code class="hljs shell">date2020年 05月 19日 星期二 16:07:17 CST</code></pre></li></ul></li></ol><ol start="6"><li><p>关闭交换分区（所以节点都执行）</p><pre><code class="hljs shell">swapoff -a（临时关闭）vim /etc/fstab（永久关闭）/dev/mapper/centos-swap swap  swap    defaults  0 0（将这行注释或删除）</code></pre><p>free -m命令检查交换分区是否关闭，主要是看Swap是否为0。</p></li></ol><pre><code class="hljs shell">free -m              total        used        free      shared  buff/cache   availableMem:           1819         142        1532           9         144        1528Swap:             0           0           0</code></pre><p>Ⅲ、SSL证书</p><ul><li><p>加密</p><pre><code>对称加密：加密解密用相同的密钥</code></pre><p>非对称加密：用公钥和私钥的密钥实现加解密<br>单向加密：只能加密，不能解密。如：MD5</p></li><li><p>ssl证书来源</p></li></ul><blockquote><p>网络第三方机构购买<br>自签证书</p></blockquote><blockquote><blockquote><p>CA：签证机构<br>自建CA<br>openssl<br>cfssl（简单一些）</p></blockquote></blockquote><p>1）etcd颁发证书</p><p><strong>a、下载cfssl工具</strong></p><pre><code class="hljs shell">wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64 -O cfssl -P /usr/local/bin/wget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64 -O cfssljson -P /usr/local/bin/wget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64 -O cfssl-certinfo -P /usr/local/bin/chmod +x /usr/local/bin/cfssl*</code></pre><p><strong>b、生成证书</strong></p><pre><code class="hljs shell">mkdir sslcd ssl生成默认的配置文件和证书签名请求文件cfssl print-defaults config &gt; ca-config.jsoncfssl print-defaults csr &gt; ca-csr.json</code></pre><p><strong>c、修改CA配置文件</strong></p><p><strong>vim ca-config.json</strong></p><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;signing&quot;</span>: &#123;        <span class="hljs-attr">&quot;default&quot;</span>: &#123;            <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;43800h&quot;</span>        &#125;,        <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;            <span class="hljs-attr">&quot;server&quot;</span>: &#123;                <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;43800h&quot;</span>,                <span class="hljs-attr">&quot;usages&quot;</span>: [                    <span class="hljs-string">&quot;signing&quot;</span>,                    <span class="hljs-string">&quot;key encipherment&quot;</span>,                    <span class="hljs-string">&quot;server auth&quot;</span>                ]            &#125;,            <span class="hljs-attr">&quot;client&quot;</span>: &#123;                <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;43800h&quot;</span>,                <span class="hljs-attr">&quot;usages&quot;</span>: [                    <span class="hljs-string">&quot;signing&quot;</span>,                    <span class="hljs-string">&quot;key encipherment&quot;</span>,                    <span class="hljs-string">&quot;client auth&quot;</span>                ]            &#125;,            <span class="hljs-attr">&quot;etcd&quot;</span>: &#123;                <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;43800h&quot;</span>,                <span class="hljs-attr">&quot;usages&quot;</span>: [                    <span class="hljs-string">&quot;signing&quot;</span>,                    <span class="hljs-string">&quot;key encipherment&quot;</span>,                    <span class="hljs-string">&quot;server auth&quot;</span>,                    <span class="hljs-string">&quot;client auth&quot;</span>                ]            &#125;        &#125;    &#125;&#125;</code></pre><p><strong>d、修改CA请求文件</strong></p><p><strong>vim ca-csr.json</strong></p><pre><code class="hljs json">&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;chengdu&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;chengdu&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;etcd&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;</code></pre><p><strong>e、生成CA证书</strong></p><pre><code class="hljs shell">cfssl gencert -initca ca-csr.json | cfssljson -bare ca</code></pre><p>该命令会在当前目录下生成<code>ca.csr</code>、<code>ca-key.pem</code>、<code>ca.pem</code>三个文件。<br>etcd证书和私钥</p><p><strong>f、创建etcd证书签名请求</strong></p><p><strong>vim etcd-csr.json</strong></p><pre><code class="hljs shell">&#123;    &quot;CN&quot;: &quot;etcd&quot;,    &quot;hosts&quot;: [      &quot;192.168.220.128&quot;,    &quot;192.168.220.129&quot;,    &quot;192.168.220.130&quot;,    &quot;192.168.220.131&quot;（#备用mater）    ],    &quot;key&quot;: &#123;        &quot;algo&quot;: &quot;rsa&quot;,        &quot;size&quot;: 2048    &#125;,    &quot;names&quot;: [        &#123;            &quot;C&quot;: &quot;CN&quot;,            &quot;ST&quot;: &quot;chengdu&quot;,            &quot;L&quot;: &quot;chengdu&quot;,            &quot;O&quot;: &quot;etcd&quot;,            &quot;OU&quot;: &quot;System&quot;        &#125;    ]&#125;</code></pre><p><strong>g、生成etcd证书和私钥</strong></p><pre><code class="hljs shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=etcd etcd-csr.json | cfssljson -bare etcd</code></pre><p>输出：</p><pre><code class="hljs shell">2020/05/19 17:51:34 [INFO] generate received request2020/05/19 17:51:34 [INFO] received CSR2020/05/19 17:51:34 [INFO] generating key: rsa-20482020/05/19 17:51:35 [INFO] encoded CSR2020/05/19 17:51:35 [INFO] signed certificate with serial number 1200445243151691394172922385949986384149133875072020/05/19 17:51:35 [WARNING] This certificate lacks a &quot;hosts&quot; field. This makes it unsuitable forwebsites. For more information see the Baseline Requirements for the Issuance and Managementof Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (https://cabforum.org);specifically, section 10.2.3 (&quot;Information Requirements&quot;).</code></pre><pre><code class="hljs shell">[root@vm001 etcd]# ll *pem-rw-------. 1 root root 1679 5月  19 17:50 ca-key.pem-rw-r--r--. 1 root root 1346 5月  19 17:50 ca.pem-rw-------. 1 root root 1679 5月  19 17:51 etcd-key.pem-rw-r--r--. 1 root root 1428 5月  19 17:51 etcd.pem</code></pre><p>2）部署etcd</p><p><a href="https://github.com/etcd-io/etcd/releases">https://github.com/etcd-io/etcd/releases</a></p><p><strong>有etcd的node上都要部署，注意文件夹的名字和位置，不是随便命名和放置的</strong>。</p><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">解压压缩包</span>mkdir /opt/etcd/&#123;bin,cfg,ssl&#125; -ptar -zxvf etcd-v3.2.12-linux-amd64.tar.gz -C ./mv etcd-v3.2.12-linux-amd64/&#123;etcd,etcdctl&#125; /opt/etcd/bin/</code></pre><p>创建etcd配置文件</p><pre><code class="hljs shell">touch etcdchmod 777 etcdcat /opt/etcd/cfg/etcd   <span class="hljs-meta">#</span><span class="bash">[Member]</span>ETCD_NAME=&quot;etcd-1&quot;ETCD_DATA_DIR=&quot;/var/lib/etcd/default.etcd&quot;ETCD_LISTEN_PEER_URLS=&quot;https://192.168.220.128:2380&quot;ETCD_LISTEN_CLIENT_URLS=&quot;https://192.168.220.128:2379&quot;<span class="hljs-meta">#</span><span class="bash">[Clustering]</span>ETCD_INITIAL_ADVERTISE_PEER_URLS=&quot;https://192.168.220.128:2380&quot;ETCD_ADVERTISE_CLIENT_URLS=&quot;https://192.168.220.128:2379&quot;ETCD_INITIAL_CLUSTER=&quot;etcd-1=https://192.168.220.128:2380,etcd-2=https://192.168.220.129:2380,etcd-3=https://192.168.220.130:2380&quot;ETCD_INITIAL_CLUSTER_TOKEN=&quot;etcd-cluster&quot;ETCD_INITIAL_CLUSTER_STATE=&quot;new&quot;<span class="hljs-meta">#</span><span class="bash">-------------------------------------解释-------------------------------------------</span><span class="hljs-meta">#</span><span class="bash">ETCD_NAME 节点名称</span><span class="hljs-meta">#</span><span class="bash">ETCD_DATA_DIR 数据目录</span><span class="hljs-meta">#</span><span class="bash">ETCD_LISTEN_PEER_URLS 集群通信监听地址</span><span class="hljs-meta">#</span><span class="bash">ETCD_LISTEN_CLIENT_URLS 客户端访问监听地址</span><span class="hljs-meta">#</span><span class="bash">ETCD_INITIAL_ADVERTISE_PEER_URLS 集群通告地址</span><span class="hljs-meta">#</span><span class="bash">ETCD_ADVERTISE_CLIENT_URLS 客户端通告地址</span><span class="hljs-meta">#</span><span class="bash">ETCD_INITIAL_CLUSTER 集群节点地址</span><span class="hljs-meta">#</span><span class="bash">ETCD_INITIAL_CLUSTER_TOKEN 集群Token</span><span class="hljs-meta">#</span><span class="bash">ETCD_INITIAL_CLUSTER_STATE 加入集群的当前状态，new是新集群，existing表示加入已有集群</span></code></pre><p>systemd管理etcd</p><pre><code class="hljs shell">touch /usr/lib/systemd/system/etcd.servicecat /usr/lib/systemd/system/etcd.service [Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.target[Service]Type=notifyEnvironmentFile=/opt/etcd/cfg/etcdExecStart=/opt/etcd/bin/etcd \--name=$&#123;ETCD_NAME&#125; \--data-dir=$&#123;ETCD_DATA_DIR&#125; \--listen-peer-urls=$&#123;ETCD_LISTEN_PEER_URLS&#125; \--listen-client-urls=$&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \--advertise-client-urls=$&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \--initial-advertise-peer-urls=$&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \--initial-cluster=$&#123;ETCD_INITIAL_CLUSTER&#125; \--initial-cluster-token=$&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \--initial-cluster-state=new \--cert-file=/opt/etcd/ssl/server.pem \--key-file=/opt/etcd/ssl/server-key.pem \--peer-cert-file=/opt/etcd/ssl/server.pem \--peer-key-file=/opt/etcd/ssl/server-key.pem \--trusted-ca-file=/opt/etcd/ssl/ca.pem \--peer-trusted-ca-file=/opt/etcd/ssl/ca.pemRestart=on-failureLimitNOFILE=65536[Install]WantedBy=multi-user.target</code></pre><p>将之前生成的证书拷贝到固定的目录</p><pre><code class="hljs shell">cp ca*pem server*pem /opt/etcd/ssl</code></pre><p>启动并设置开机启动</p><pre><code class="hljs shell">systemctl start etcdsystemctl enable etcd</code></pre><p><strong>注意：以上步骤需要在所有要安装etcd的node上执行一次，不是只在master node上执行，可以通过scp命令发送到其他node，需要改一下etcd的配置文件，节点名和节点ip。</strong></p><p>在都部署完成后，检查一下集群的状态(若是出现cluster is healthy，说明部署成功！)</p><p>检查</p><pre><code class="hljs shell">HOST_1=192.168.220.128HOST_2=192.168.220.129HOST_3=192.168.220.130ENDPOINTS=$HOST_1:2379,$HOST_2:2379,$HOST_3:2379./etcdctl --endpoints=$ENDPOINTS member listError:  context deadline exceeded./etcdctl --endpoints=$ENDPOINTS --cacert=/opt/etcd/ssl/ca.pem --key=/opt/etcd/ssl/server-key.pem  --cert=/opt/etcd/ssl/server.pem  endpoint health192.168.220.128:2379 is healthy: successfully committed proposal: took = 8.495621ms192.168.220.130:2379 is healthy: successfully committed proposal: took = 8.654833ms192.168.220.129:2379 is healthy: successfully committed proposal: took = 12.661909ms</code></pre><p><strong>参考文档：<a href="https://blog.csdn.net/god_wot/article/details/77854093">https://blog.csdn.net/god_wot/article/details/77854093</a></strong></p><p><strong>参考文档:<a href="http://www.codedog.fun/2020/04/12/">http://www.codedog.fun/2020/04/12/</a></strong></p><h3 id="Kubernetes-Master-二进制部署"><a href="#Kubernetes-Master-二进制部署" class="headerlink" title="Kubernetes Master 二进制部署"></a>Kubernetes Master 二进制部署</h3><ul><li><p>kube-apiserver</p></li><li><p>kube-controller-manager</p></li><li><p>kube-scheduler</p><blockquote><p>kubernetes master 节点运行如下组件： kube-apiserver kube-scheduler kube-controller-manager. 其中kube-scheduler 和 kube-controller-manager 可以以集群模式运行，通过 leader 选举产生一个工作进程，其它进程处于阻塞模式，master高可用模式下可用</p></blockquote></li></ul><h6 id="github-官网"><a href="#github-官网" class="headerlink" title="github 官网"></a>github 官网</h6><p><a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a></p><h2 id="安装步骤"><a href="#安装步骤" class="headerlink" title="安装步骤"></a>安装步骤</h2><ul><li>下载文件</li><li>制作证书</li><li>创建TLS Bootstrapping Token</li><li>部署apiserver组件</li><li>部署kube-scheduler组件</li><li>部署kube-controller-manager组件</li><li>验证服务</li></ul><h3 id="下载文件-解压缩"><a href="#下载文件-解压缩" class="headerlink" title="下载文件,解压缩"></a>下载文件,解压缩</h3><pre><code class="hljs shell">wget https://dl.k8s.io/v1.13.6/kubernetes-server-linux-amd64.tar.gzmkdir /opt/kubernetes/&#123;bin,cfg,ssl&#125; -ptar zxf kubernetes-server-linux-amd64.tar.gz cd kubernetes/server/bin/cp kube-scheduler kube-apiserver kube-controller-manager /opt/kubernetes/bin/[root@vm001 bin]# cp kubectl /usr/bin/</code></pre><h3 id="制作kubernetes-ca证书"><a href="#制作kubernetes-ca证书" class="headerlink" title="制作kubernetes ca证书"></a>制作kubernetes ca证书</h3><pre><code class="hljs json">cd  /opt/kubernetes/ssl/cat &lt;&lt; EOF | tee /opt/kubernetes/ssl/ca-config.json&#123;  <span class="hljs-attr">&quot;signing&quot;</span>: &#123;    <span class="hljs-attr">&quot;default&quot;</span>: &#123;      <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>    &#125;,    <span class="hljs-attr">&quot;profiles&quot;</span>: &#123;      <span class="hljs-attr">&quot;kubernetes&quot;</span>: &#123;         <span class="hljs-attr">&quot;expiry&quot;</span>: <span class="hljs-string">&quot;87600h&quot;</span>,         <span class="hljs-attr">&quot;usages&quot;</span>: [            <span class="hljs-string">&quot;signing&quot;</span>,            <span class="hljs-string">&quot;key encipherment&quot;</span>,            <span class="hljs-string">&quot;server auth&quot;</span>,            <span class="hljs-string">&quot;client auth&quot;</span>        ]      &#125;    &#125;  &#125;&#125;EOFcat &lt;&lt; EOF | tee /opt/kubernetes/ssl/ca-csr.json&#123;    <span class="hljs-attr">&quot;CN&quot;</span>: <span class="hljs-string">&quot;kubernetes&quot;</span>,    <span class="hljs-attr">&quot;key&quot;</span>: &#123;        <span class="hljs-attr">&quot;algo&quot;</span>: <span class="hljs-string">&quot;rsa&quot;</span>,        <span class="hljs-attr">&quot;size&quot;</span>: <span class="hljs-number">2048</span>    &#125;,    <span class="hljs-attr">&quot;names&quot;</span>: [        &#123;            <span class="hljs-attr">&quot;C&quot;</span>: <span class="hljs-string">&quot;CN&quot;</span>,            <span class="hljs-attr">&quot;L&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-attr">&quot;ST&quot;</span>: <span class="hljs-string">&quot;Beijing&quot;</span>,            <span class="hljs-attr">&quot;O&quot;</span>: <span class="hljs-string">&quot;k8s&quot;</span>,            <span class="hljs-attr">&quot;OU&quot;</span>: <span class="hljs-string">&quot;System&quot;</span>        &#125;    ]&#125;EOF</code></pre><p>生成ca证书</p><pre><code class="hljs shell">cfssl gencert -initca ca-csr.json | cfssljson -bare ca -</code></pre><blockquote><p>2019/05/28 14:47:18 [INFO] generating a new CA key and certificate from CSR<br>2019/05/28 14:47:18 [INFO] generate received request<br>2019/05/28 14:47:18 [INFO] received CSR<br>2019/05/28 14:47:18 [INFO] generating key: rsa-2048<br>2019/05/28 14:47:18 [INFO] encoded CSR<br>2019/05/28 14:47:19 [INFO] signed certificate with serial number 34219464473634319112180195944445301722929678647</p></blockquote><p>制作apiserver证书</p><pre><code class="hljs shell">cat &lt;&lt; EOF | tee server-csr.json</code></pre><blockquote><p>{<br>    “CN”: “kubernetes”,<br>    “hosts”: [<br>          “10.0.0.1”,<br>          “127.0.0.1”,<br>          “192.168.220.128”,<br>          “192.168.220.129”,<br>          “192.168.220.130”,<br>          “kubernetes”,<br>          “kubernetes.default”,<br>          “kubernetes.default.svc”,<br>          “kubernetes.default.svc.cluster”,<br>          “kubernetes.default.svc.cluster.local”<br>    ],<br>    “key”: {<br>        “algo”: “rsa”,<br>        “size”: 2048<br>    },<br>    “names”: [<br>        {<br>            “C”: “CN”,<br>            “L”: “Beijing”,<br>            “ST”: “Beijing”,<br>            “O”: “k8s”,<br>            “OU”: “System”<br>        }<br>    ]<br>}<br>EOF</p></blockquote><pre><code class="hljs shell">cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes server-csr.json | cfssljson -bare server</code></pre><blockquote><p>2019/05/28 15:03:31 [INFO] generate received request<br>2019/05/28 15:03:31 [INFO] received CSR<br>2019/05/28 15:03:31 [INFO] generating key: rsa-2048<br>2019/05/28 15:03:31 [INFO] encoded CSR<br>2019/05/28 15:03:31 [INFO] signed certificate with serial number 114040551556369232239873744650692828468613738631<br>2019/05/28 15:03:31 [WARNING] This certificate lacks a “hosts” field. This makes it unsuitable for<br>websites. For more information see the Baseline Requirements for the Issuance and Management<br>of Publicly-Trusted Certificates, v.1.1.6, from the CA/Browser Forum (<a href="https://cabforum.org/">https://cabforum.org</a>);<br>specifically, section 10.2.3 (“Information Requirements”).</p></blockquote><pre><code class="hljs shell">lsca-config.json  ca.csr  ca-csr.json  ca-key.pem  ca.pem  server.csr  server-csr.json  server-key.pem  server.pem</code></pre><p><strong>server-csr.json文件中的hosts配置,需要”192.168.220.128”,  “192.168.220.129”,”192.168.220.130”, 这几个IP是自己设置的,其他的都是内置的,无需改动,其中我们k8smaster的ip,如果高可用的话,几个master的ip,lb的ip都需要设置,否则无法连接apiserver.</strong></p><h3 id="创建TLS-Bootstrapping-Token"><a href="#创建TLS-Bootstrapping-Token" class="headerlink" title="创建TLS Bootstrapping Token"></a>创建TLS Bootstrapping Token</h3><pre><code class="hljs shell">BOOTSTRAP_TOKEN=$(head -c 16 /dev/urandom | od -An -t x | tr -d &#x27; &#x27;)</code></pre><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /opt/kubernetes/cfg/token.csv<span class="hljs-meta">&gt;</span><span class="bash"> <span class="hljs-variable">$&#123;BOOTSTRAP_TOKEN&#125;</span>,kubelet-bootstrap,10001,<span class="hljs-string">&quot;system:kubelet-bootstrap&quot;</span></span><span class="hljs-meta">&gt;</span><span class="bash"> EOF</span>7d558bb3a5206cf78f881de7d7b82ca6,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</code></pre><pre><code class="hljs shell">cat /opt/kubernetes/cfg/token.csv7d558bb3a5206cf78f881de7d7b82ca6,kubelet-bootstrap,10001,&quot;system:kubelet-bootstrap&quot;</code></pre><h3 id="部署kube-apiserver组件"><a href="#部署kube-apiserver组件" class="headerlink" title="部署kube-apiserver组件"></a>部署kube-apiserver组件</h3><h4 id="1-创建kube-apiserver配置文件"><a href="#1-创建kube-apiserver配置文件" class="headerlink" title="1. 创建kube-apiserver配置文件"></a>1. 创建kube-apiserver配置文件</h4><p>Apiserver配置文件里面需要配置的有etcd-servers地址,bind-address,advertise-address都是当前master节点的IP地址.token-auth-file和各种pem认证文件,需要把对应的文件位置输入即可.</p><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /opt/kubernetes/cfg/kube-apiserverKUBE_APISERVER_OPTS=&quot;--logtostderr=true \\--v=4 \\--etcd-servers=https://192.168.220.128:2379,https://192.168.220.129:2379,https://192.168.220.130:2379 \\--bind-address=192.168.220.128 \\--secure-port=6443 \\--advertise-address=192.168.220.128 \\--allow-privileged=true \\--service-cluster-ip-range=10.0.0.0/24 \\--enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction \\--authorization-mode=RBAC,Node \\--kubelet-https=true \\--enable-bootstrap-token-auth \\--token-auth-file=/opt/kubernetes/cfg/token.csv \\--service-node-port-range=30000-50000 \\--tls-cert-file=/opt/kubernetes/ssl/server.pem  \\--tls-private-key-file=/opt/kubernetes/ssl/server-key.pem \\--client-ca-file=/opt/kubernetes/ssl/ca.pem \\--service-account-key-file=/opt/kubernetes/ssl/ca-key.pem \\--etcd-cafile=/opt/etcd/ssl/ca.pem \\--etcd-certfile=/opt/etcd/ssl/server.pem \\--etcd-keyfile=/opt/etcd/ssl/server-key.pem&quot;EOF</code></pre><h4 id="2-创建apiserver-systemd文件"><a href="#2-创建apiserver-systemd文件" class="headerlink" title="2.创建apiserver systemd文件"></a>2.创建apiserver systemd文件</h4><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /usr/lib/systemd/system/kube-apiserver.service[Unit]Description=Kubernetes API ServerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-apiserverExecStart=/opt/kubernetes/bin/kube-apiserver \$KUBE_APISERVER_OPTSRestart=on-failure[Install]</code></pre><h4 id="3-启动服务"><a href="#3-启动服务" class="headerlink" title="3.启动服务"></a>3.启动服务</h4><pre><code class="hljs shell">systemctl daemon-reloadsystemctl enable kube-apiserversystemctl start kube-apiserver</code></pre><pre><code class="hljs shell">ps -ef |grep kube-apiserverroot      13975      1 10 14:45 ?        00:22:21 /opt/kubernetes/bin/kube-apiserver --logtostderr=true --v=2 --log-dir=/opt/kubernetes/logs --etcd-servers=https://192.168.220.128:2379,https://192.168.220.129:2379,https://192.168.220.130:2379 --bind-address=192.168.220.128 --secure-port=6443 --advertise-address=192.168.220.128 --allow-privileged=true --service-cluster-ip-range=10.0.0.0/24 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --kubelet-https=true --enable-bootstrap-token-auth --token-auth-file=/opt/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/opt/kubernetes/ssl/server.pem --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem --client-ca-file=/opt/kubernetes/ssl/ca.pem --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem --etcd-cafile=/opt/etcd/ssl/ca.pem --etcd-certfile=/opt/etcd/ssl/server.pem --etcd-keyfile=/opt/etcd/ssl/server-key.pemroot      26176   1540  0 18:23 pts/0    00:00:00 grep --color=auto kube-apiserver</code></pre><pre><code class="hljs shell">ps -ef |grep -v grep |grep kube-apiserver root      13975      1 10 14:45 ?        00:22:14 /opt/kubernetes/bin/kube-apiserver --logtostderr=true --v=2 --log-dir=/opt/kubernetes/logs --etcd-servers=https://192.168.220.128:2379,https://192.168.220.129:2379,https://192.168.220.130:2379 --bind-address=192.168.220.128 --secure-port=6443 --advertise-address=192.168.220.128 --allow-privileged=true --service-cluster-ip-range=10.0.0.0/24 --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota,NodeRestriction --authorization-mode=RBAC,Node --kubelet-https=true --enable-bootstrap-token-auth --token-auth-file=/opt/kubernetes/cfg/token.csv --service-node-port-range=30000-50000 --tls-cert-file=/opt/kubernetes/ssl/server.pem --tls-private-key-file=/opt/kubernetes/ssl/server-key.pem --client-ca-file=/opt/kubernetes/ssl/ca.pem --service-account-key-file=/opt/kubernetes/ssl/ca-key.pem --etcd-cafile=/opt/etcd/ssl/ca.pem --etcd-certfile=/opt/etcd/ssl/server.pem --etcd-keyfile=/opt/etcd/ssl/server-key.pem</code></pre><pre><code class="hljs shell">netstat -tulpn |grep kube-apiservetcp        0      0 10.0.52.13:6443         0.0.0.0:*               LISTEN      19404/kube-apiserve tcp        0      0 127.0.0.1:8080          0.0.0.0:*               LISTEN      19404/kube-apiserve</code></pre><pre><code class="hljs shell">systemctl status kube-apiserver● kube-apiserver.service - Kubernetes API Server   Loaded: loaded (/usr/lib/systemd/system/kube-apiserver.service; enabled; vendor preset: disabled)   Active: active (running) since 四 2020-05-21 14:45:10 CST; 3h 38min ago     Docs: https://github.com/kubernetes/kubernetes Main PID: 13975 (kube-apiserver)   CGroup: /system.slice/kube-apiserver.service           └─13975 /opt/kubernetes/bin/kube-apiserver --logtostderr=true --v=2 --log-dir=/opt/kubernetes/logs --etcd-servers=https://192.168.220.128:237...5月 21 18:22:51 vm001 kube-apiserver[13975]: E0521 18:22:51.869091   13975 available_controller.go:416] v1beta1.metrics.k8s.io failed with: failing or ...5月 21 18:23:16 vm001 kube-apiserver[13975]: E0521 18:23:16.882526   13975 available_controller.go:416] v1beta1.metrics.k8s.io failed with: failing or ...5月 21 18:23:17 vm001 kube-apiserver[13975]: I0521 18:23:17.864672   13975 controller.go:107] OpenAPI AggregationController: Processing item v1...s.k8s.io5月 21 18:23:17 vm001 kube-apiserver[13975]: W0521 18:23:17.864836   13975 handler_proxy.go:99] no RequestInfo found in the context5月 21 18:23:17 vm001 kube-apiserver[13975]: E0521 18:23:17.864934   13975 controller.go:114] loading OpenAPI spec for &quot;v1beta1.metrics.k8s.io&quot;...vailable5月 21 18:23:17 vm001 kube-apiserver[13975]: , Header: map[Content-Type:[text/plain; charset=utf-8] X-Content-Type-Options:[nosniff]]5月 21 18:23:17 vm001 kube-apiserver[13975]: I0521 18:23:17.864955   13975 controller.go:127] OpenAPI AggregationController: action for item v1...Requeue.5月 21 18:23:21 vm001 kube-apiserver[13975]: E0521 18:23:21.884702   13975 available_controller.go:416] v1beta1.metrics.k8s.io failed with: failing or ...5月 21 18:23:51 vm001 kube-apiserver[13975]: E0521 18:23:51.872210   13975 available_controller.go:416] v1beta1.metrics.k8s.io failed with: failing or ...5月 21 18:23:56 vm001 kube-apiserver[13975]: E0521 18:23:56.873737   13975 available_controller.go:416] v1beta1.metrics.k8s.io failed with: failing or ...Hint: Some lines were ellipsized, use -l to show in full.</code></pre><h3 id="部署kube-scheduler组件"><a href="#部署kube-scheduler组件" class="headerlink" title="部署kube-scheduler组件"></a>部署kube-scheduler组件</h3><h4 id="1-创建kube-scheduler配置文件"><a href="#1-创建kube-scheduler配置文件" class="headerlink" title="1.创建kube-scheduler配置文件"></a>1.创建kube-scheduler配置文件</h4><p>–master: kube-scheduler 使用它连接 kube-apiserver； –leader-elect=true：集群运行模式，启用选举功能；被选为 leader 的节点负责处理工作，其它节点为阻塞状态；</p><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /opt/kubernetes/cfg/kube-schedulerKUBE_SCHEDULER_OPTS=&quot;--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect&quot;EOF</code></pre><h4 id="2-创建kube-scheduler-systemd文件"><a href="#2-创建kube-scheduler-systemd文件" class="headerlink" title="2.创建kube-scheduler systemd文件"></a>2.创建kube-scheduler systemd文件</h4><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /usr/lib/systemd/system/kube-scheduler.service[Unit]Description=Kubernetes SchedulerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-schedulerExecStart=/opt/kubernetes/bin/kube-scheduler \$KUBE_SCHEDULER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF</code></pre><h4 id="3-启动-amp-验证"><a href="#3-启动-amp-验证" class="headerlink" title="3.启动&amp;验证"></a>3.启动&amp;验证</h4><pre><code class="hljs shell">systemctl daemon-reloadsystemctl enable kube-schedulersystemctl start kube-scheduler</code></pre><pre><code class="hljs shell">systemctl status kube-scheduler.service</code></pre><pre><code class="hljs shell">● kube-scheduler.service - Kubernetes Scheduler   Loaded: loaded (/usr/lib/systemd/system/kube-scheduler.service; enabled; vendor preset: disabled)   Active: active (running) since 四 2020-05-21 11:19:54 CST; 7h ago     Docs: https://github.com/kubernetes/kubernetes Main PID: 3228 (kube-scheduler)   CGroup: /system.slice/kube-scheduler.service           └─3228 /opt/kubernetes/bin/kube-scheduler --logtostderr=true --v=4 --master=127.0.0.1:8080 --leader-elect5月 21 18:20:04 vm001 kube-scheduler[3228]: I0521 18:20:04.532912    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:21:15 vm001 kube-scheduler[3228]: I0521 18:21:15.543870    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:21:19 vm001 kube-scheduler[3228]: I0521 18:21:19.531873    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:22:35 vm001 kube-scheduler[3228]: I0521 18:22:35.425484    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:22:51 vm001 kube-scheduler[3228]: I0521 18:22:51.417292    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:23:14 vm001 kube-scheduler[3228]: I0521 18:23:14.431914    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:23:17 vm001 kube-scheduler[3228]: I0521 18:23:17.576426    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:24:12 vm001 kube-scheduler[3228]: I0521 18:24:12.543532    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:25:11 vm001 kube-scheduler[3228]: I0521 18:25:11.537550    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...received5月 21 18:26:19 vm001 kube-scheduler[3228]: I0521 18:26:19.541437    3228 reflector.go:383] k8s.io/client-go/informers/factory.go:134: Watch cl...receivedHint: Some lines were ellipsized, use -l to show in full.</code></pre><h3 id="部署kube-controller-manager组件"><a href="#部署kube-controller-manager组件" class="headerlink" title="部署kube-controller-manager组件"></a>部署kube-controller-manager组件</h3><h4 id="1-创建kube-controller-manager配置文件"><a href="#1-创建kube-controller-manager配置文件" class="headerlink" title="1.创建kube-controller-manager配置文件"></a>1.创建kube-controller-manager配置文件</h4><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /opt/kubernetes/cfg/kube-controller-managerKUBE_CONTROLLER_MANAGER_OPTS=&quot;--logtostderr=true \\--v=4 \\--master=127.0.0.1:8080 \\--leader-elect=true \\--address=127.0.0.1 \\--service-cluster-ip-range=10.0.0.0/24 \\--cluster-name=kubernetes \\--cluster-signing-cert-file=/opt/kubernetes/ssl/ca.pem \\--cluster-signing-key-file=/opt/kubernetes/ssl/ca-key.pem  \\--root-ca-file=/opt/kubernetes/ssl/ca.pem \\--service-account-private-key-file=/opt/kubernetes/ssl/ca-key.pem \\--experimental-cluster-signing-duration=87600h0m0s&quot;EOF</code></pre><h4 id="2-创建kube-controller-manager-systemd文件"><a href="#2-创建kube-controller-manager-systemd文件" class="headerlink" title="2.创建kube-controller-manager systemd文件"></a>2.创建kube-controller-manager systemd文件</h4><pre><code class="hljs shell">cat &lt;&lt; EOF | tee /usr/lib/systemd/system/kube-controller-manager.service[Unit]Description=Kubernetes Controller ManagerDocumentation=https://github.com/kubernetes/kubernetes[Service]EnvironmentFile=-/opt/kubernetes/cfg/kube-controller-managerExecStart=/opt/kubernetes/bin/kube-controller-manager \$KUBE_CONTROLLER_MANAGER_OPTSRestart=on-failure[Install]WantedBy=multi-user.targetEOF</code></pre><h4 id="3-启动-amp-验证-1"><a href="#3-启动-amp-验证-1" class="headerlink" title="3.启动&amp;验证"></a>3.启动&amp;验证</h4><pre><code class="hljs shell">systemctl daemon-reloadsystemctl enable kube-controller-managersystemctl start kube-controller-manager</code></pre><pre><code class="hljs shell">systemctl status kube-controller-manager</code></pre><pre><code class="hljs shell">● kube-controller-manager.service - Kubernetes Controller Manager   Loaded: loaded (/usr/lib/systemd/system/kube-controller-manager.service; enabled; vendor preset: disabled)   Active: active (running) since 四 2020-05-21 11:23:14 CST; 7h ago     Docs: https://github.com/kubernetes/kubernetes Main PID: 3457 (kube-controller)   CGroup: /system.slice/kube-controller-manager.service           └─3457 /opt/kubernetes/bin/kube-controller-manager --logtostderr=true --v=2 --log-dir=/opt/kubernetes/kube-controller/logs --master=127.0.0.1...5月 21 18:28:00 vm001 kube-controller-manager[3457]: W0521 18:28:00.196691    3457 garbagecollector.go:640] failed to discover some groups: map...request]5月 21 18:28:19 vm001 kube-controller-manager[3457]: E0521 18:28:19.222236    3457 resource_quota_controller.go:407] unable to retrieve the com... request5月 21 18:28:32 vm001 kube-controller-manager[3457]: W0521 18:28:32.229779    3457 garbagecollector.go:640] failed to discover some groups: map...request]5月 21 18:28:49 vm001 kube-controller-manager[3457]: E0521 18:28:49.477449    3457 resource_quota_controller.go:407] unable to retrieve the com... request5月 21 18:29:04 vm001 kube-controller-manager[3457]: W0521 18:29:04.237698    3457 garbagecollector.go:640] failed to discover some groups: map...request]5月 21 18:29:19 vm001 kube-controller-manager[3457]: E0521 18:29:19.730132    3457 resource_quota_controller.go:407] unable to retrieve the com... request5月 21 18:29:36 vm001 kube-controller-manager[3457]: W0521 18:29:36.241963    3457 garbagecollector.go:640] failed to discover some groups: map...request]5月 21 18:29:49 vm001 kube-controller-manager[3457]: E0521 18:29:49.988909    3457 resource_quota_controller.go:407] unable to retrieve the com... request5月 21 18:30:08 vm001 kube-controller-manager[3457]: W0521 18:30:08.248640    3457 garbagecollector.go:640] failed to discover some groups: map...request]5月 21 18:30:20 vm001 kube-controller-manager[3457]: E0521 18:30:20.241216    3457 resource_quota_controller.go:407] unable to retrieve the com... requestHint: Some lines were ellipsized, use -l to show in full.</code></pre><h3 id="验证master服务状态"><a href="#验证master服务状态" class="headerlink" title="验证master服务状态"></a>验证master服务状态</h3><pre><code class="hljs shell">kubectl get cs</code></pre><blockquote><p>NAME                 AGE<br>scheduler            <unknown><br>controller-manager   <unknown><br>etcd-2               <unknown><br>etcd-1               <unknown><br>etcd-0               <unknown></p></blockquote><p>如果出现如上界面,表示Master安装成功!</p><p>master参考文档：<a href="https://www.jianshu.com/p/bc20454096a1">https://www.jianshu.com/p/bc20454096a1</a></p><p>node参考文档：<a href="https://www.jianshu.com/p/983f56bea420">https://www.jianshu.com/p/983f56bea420</a></p><p>kuboard参考文档：[<a href="https://www.kuboard.cn/install/install-dashboard.html#%E5%9C%A8%E7%BA%BF%E4%BD%93%E9%AA%8C">https://www.kuboard.cn/install/install-dashboard.html#%E5%9C%A8%E7%BA%BF%E4%BD%93%E9%AA%8C</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Kubernetes核心概念介绍</title>
    <link href="/2020/09/02/Kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/"/>
    <url>/2020/09/02/Kubernetes%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E4%BB%8B%E7%BB%8D/</url>
    
    <content type="html"><![CDATA[<h3 id="什么是Kubernetes"><a href="#什么是Kubernetes" class="headerlink" title="什么是Kubernetes"></a><em>什么是Kubernetes</em></h3><p>​    <a href="https://www.kubernetes.org.cn/"><strong>Kubernetes</strong></a>是一个开源的，用于管理云平台中多个主机上的容器化的应用，Kubernetes的目标是让部署容器化的应用简单并且高效（powerful）,Kubernetes提供了应用部署，规划，更新，维护的一种机制。</p><h3 id="Kubernetes的设计架构"><a href="#Kubernetes的设计架构" class="headerlink" title="Kubernetes的设计架构"></a><em>Kubernetes的设计架构</em></h3><p>​    <a href="https://www.kubernetes.org.cn/"><strong>Kubernetes</strong></a>集群包含有节点代理kubelet和Master组件(APIs, scheduler, etc)，一切都基于分布式的存储系统。下面这张图是Kubernetes的架构图。</p><p><img src="https://www.kubernetes.org.cn/img/2016/10/20161028141542.jpg"></p><h3 id="Kubernetes节点"><a href="#Kubernetes节点" class="headerlink" title="Kubernetes节点"></a>Kubernetes节点</h3><hr><p>在这张系统架构图中，我们把服务分为运行在工作节点上的服务和组成集群级别控制板的服务。</p><p>Kubernetes节点有运行应用容器必备的服务，而这些都是受Master的控制。</p><p>每次个节点上当然都要运行Docker。Docker来负责所有具体的映像下载和容器运行。</p><p>Kubernetes主要由以下几个核心组件组成：</p><ul><li>etcd保存了整个集群的状态；</li><li>apiserver提供了资源操作的唯一入口，并提供认证、授权、访问控制、API注册和发现等机制；</li><li>controller manager负责维护集群的状态，比如故障检测、自动扩展、滚动更新等；</li><li>scheduler负责资源的调度，按照预定的调度策略将Pod调度到相应的机器上；</li><li>kubelet负责维护容器的生命周期，同时也负责Volume（CVI）和网络（CNI）的管理；</li><li>Container runtime负责镜像管理以及Pod和容器的真正运行（CRI）；</li><li>kube-proxy负责为Service提供cluster内部的服务发现和负载均衡；</li></ul><p>除了核心组件，还有一些推荐的Add-ons：</p><ul><li>kube-dns负责为整个集群提供DNS服务</li><li>Ingress Controller为服务提供外网入口</li><li>Heapster提供资源监控</li><li>Dashboard提供GUI</li><li>Federation提供跨可用区的集群</li><li>Fluentd-elasticsearch提供集群日志采集、存储与查询</li></ul><h4 id="kubelet"><a href="#kubelet" class="headerlink" title="kubelet"></a>kubelet</h4><p>​    kubelet负责管理<a href="https://www.kubernetes.org.cn/kubernetes-pod">pods</a>和它们上面的容器，images镜像、volumes、etc。</p><h4 id="kube-proxy"><a href="#kube-proxy" class="headerlink" title="kube-proxy"></a>kube-proxy</h4><p>​    每一个节点也运行一个简单的网络代理和负载均衡（详见<a href="https://github.com/kubernetes/kubernetes/wiki/Services-FAQ">services FAQ</a> )（PS:官方 英文）。 正如Kubernetes API里面定义的这些服务（详见<a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/user-guide/services.md">the services doc</a>）（PS:官方 英文）也可以在各种终端中以轮询的方式做一些简单的TCP和UDP传输。</p><p>​    服务端点目前是通过DNS或者环境变量( Docker-links-compatible 和 Kubernetes{FOO}_SERVICE_HOST 及 {FOO}_SERVICE_PORT 变量都支持)。这些变量由服务代理所管理的端口来解析。</p><h4 id="Kubernetes控制面板"><a href="#Kubernetes控制面板" class="headerlink" title="Kubernetes控制面板"></a>Kubernetes控制面板</h4><p>​    Kubernetes控制面板可以分为多个部分。目前它们都运行在一个<em>master</em> 节点，然而为了达到高可用性，这需要改变。不同部分一起协作提供一个统一的关于集群的视图。</p><h4 id="etcd"><a href="#etcd" class="headerlink" title="etcd"></a>etcd</h4><p>​    所有master的持续状态都存在etcd的一个实例中。这可以很好地存储配置数据。因为有watch(观察者)的支持，各部件协调中的改变可以很快被察觉。</p><h4 id="Kubernetes-API-Server"><a href="#Kubernetes-API-Server" class="headerlink" title="Kubernetes API Server"></a>Kubernetes API Server</h4><p>​    API服务提供<a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/api.md">Kubernetes API</a> （PS:官方 英文）的服务。这个服务试图通过把所有或者大部分的业务逻辑放到不两只的部件中从而使其具有CRUD特性。它主要处理REST操作，在etcd中验证更新这些对象（并最终存储）。</p><h4 id="Scheduler"><a href="#Scheduler" class="headerlink" title="Scheduler"></a>Scheduler</h4><p>​    调度器把未调度的pod通过binding api绑定到节点上。调度器是可插拔的，并且我们期待支持多集群的调度，未来甚至希望可以支持用户自定义的调度器。</p><h4 id="Kubernetes控制管理服务器"><a href="#Kubernetes控制管理服务器" class="headerlink" title="Kubernetes控制管理服务器"></a>Kubernetes控制管理服务器</h4><p>​    所有其它的集群级别的功能目前都是由控制管理器所负责。例如，端点对象是被端点控制器来创建和更新。这些最终可以被分隔成不同的部件来让它们独自的可插拔。</p><p>​    <a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/user-guide/replication-controller.md">replicationcont</a><a href="https://github.com/kubernetes/kubernetes/blob/release-1.2/docs/user-guide/replication-controller.md">roller</a>是一种建立于简单的 <a href="https://www.kubernetes.org.cn/kubernetes-pod">pod</a> API之上的一种机制。一旦实现，我们最终计划把这变成一种通用的插件机制。</p><h3 id="Kubernetes的核心概念"><a href="#Kubernetes的核心概念" class="headerlink" title="Kubernetes的核心概念"></a>Kubernetes的核心概念</h3><h4 id="Pod"><a href="#Pod" class="headerlink" title="Pod"></a>Pod</h4><p>​    Pod是k8s是能够被运行的最小单元。一个pod里面运行多个容器，他们共享<code>UTS+NET+IPC</code>名称空间。一个pod运行多个容器，又叫：<code>边车模式（SideCar）</code></p><h4 id="pod控制器"><a href="#pod控制器" class="headerlink" title="pod控制器"></a>pod控制器</h4><p>​    pod控制器是pod启动的一种模板，用来保证k8s里启动的pod始终按照人们的预期运行（副本数、生命周期、健康状态检查……）</p><p>​    K8S内提供的众多Pod控制器，常用的有一下几种：</p><blockquote><p>Depoloyment<br>DaemonSet<br>ReplicaSet<br>StatefulSet<br>Job<br>Cronjob</p></blockquote><h4 id="Service"><a href="#Service" class="headerlink" title="Service"></a>Service</h4><p>​    在k8s里面，虽然每个pod都会被分配一个单独的IP地址，但这个IP地址会随着pod的销毁而消失。Service（服务）就是解决这个问题的核心概念。</p><p>​    一个Service可以看作是一组提供相同服务的pod的对外访问接口，Service的作用哪些pod是通过标签选择器来定义的。</p><h4 id="Ingress"><a href="#Ingress" class="headerlink" title="Ingress"></a>Ingress</h4><p>​    Ingress是k8s集群里工作在OSI网络参考模型下，第七层的应该，对外暴露的接口。Service只能进行L4流量调度，表现形式<code>ip+port</code>。Ingress则可以调度不同业务域，不同URL访问路径的业务流量。</p><h4 id="Name"><a href="#Name" class="headerlink" title="Name"></a>Name</h4><p>​    由于k8s内部，使用“资源”来定义每一种逻辑（功能）故每种“资源”都应该有自己的“”名称。</p><h4 id="Namespace"><a href="#Namespace" class="headerlink" title="Namespace"></a>Namespace</h4><p>​    随着项目增多、人员增加、集群规模扩大，需要一种能够隔离k8s内各种“资源”的方法，这就是名称空间。<br>名称空间可以理解为k8s内部的虚拟集群组。k8s里默认的名称空间有：<code>default、kube-system、kube-public</code>。查询k8s里特定的“资源”要带上相应的名称空间。</p><h4 id="Label"><a href="#Label" class="headerlink" title="Label"></a>Label</h4><p>​    标签是k8s特色的管理方式，便于分类管理资源对象。一个标签可以对应多个资源，一个资源也可以对应多个标签，他们是多对多的关系。<br>​    标签的组成：<code>key=value</code><br>​    与标签类似的，还有一种“注解”（<code>annotations</code>）。</p><h4 id="Label选择器"><a href="#Label选择器" class="headerlink" title="Label选择器"></a>Label选择器</h4><p>​    资源打上标签后，可以使用标签选择器过滤掉指定的标签。标签选择器目前有两个：基于等值关系（等于、不等于）和基于集合关系（属于、不属于、存在）。许多资源支持内嵌标签选择器字段：<code>matchLabels</code>、<code>matchExpressions</code>。    </p><h4 id="RBAC访问授权"><a href="#RBAC访问授权" class="headerlink" title="RBAC访问授权"></a>RBAC访问授权</h4><p>​    K8s在1.3版本中发布了alpha版的基于角色的访问控制（Role-based Access Control，RBAC）的授权模式。相对于基于属性的访问控制（Attribute-based Access Control，ABAC），RBAC主要是引入了角色（Role）和角色绑定（RoleBinding）的抽象概念。在ABAC中，K8s集群中的访问策略只能跟用户直接关联；而在RBAC中，访问策略可以跟某个角色关联，具体的用户在跟一个或多个角色相关联。显然，RBAC像其他新功能一样，每次引入新功能，都会引入新的API对象，从而引入新的概念抽象，而这一新的概念抽象一定会使集群服务管理和使用更容易扩展和重用。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>从K8s的系统架构、技术概念和设计理念，我们可以看到K8s系统最核心的两个设计理念：一个是<code>容错性</code>，一个是<code>易扩展性</code>。容错性实际是保证K8s系统稳定性和安全性的基础，易扩展性是保证K8s对变更友好，可以快速迭代增加新功能的基础。</p><h3 id="参考文档"><a href="#参考文档" class="headerlink" title="参考文档"></a>参考文档</h3><p><a href="http://www.infoq.com/cn/articles/kubernetes-and-cloud-native-applications-part01">Kubernetes与云原生应用</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Kubernetes</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>基于Django的cmdb运维资产管理系统</title>
    <link href="/2020/08/31/%E5%9F%BA%E4%BA%8EDjango%E7%9A%84cmdb%E8%BF%90%E7%BB%B4%E8%B5%84%E4%BA%A7%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/"/>
    <url>/2020/08/31/%E5%9F%BA%E4%BA%8EDjango%E7%9A%84cmdb%E8%BF%90%E7%BB%B4%E8%B5%84%E4%BA%A7%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/</url>
    
    <content type="html"><![CDATA[<h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p><code>python 3.7.3</code>   <code>django 2.2.5</code>  <code>simpleui 2.9</code></p><h3 id="docker部署"><a href="#docker部署" class="headerlink" title="docker部署"></a>docker部署</h3><pre><code class="hljs shell">docker pull lghost/cmdb # 拉取镜像到本地docker run -d -p 8080:8080  --name cmdb lghost/cmdb:latest<span class="hljs-meta">#</span><span class="bash"> 运行容器</span>docker exec -it cmdb  python manage.py createsuperuser<span class="hljs-meta">#</span><span class="bash"> 设置密码</span></code></pre><h3 id="手动部署"><a href="#手动部署" class="headerlink" title="手动部署"></a>手动部署</h3><pre><code class="hljs shell">cd cmdbecho env_django &gt;&gt; .gitignore # 排除env环境上传至gitpython3 -m venv env_django<span class="hljs-meta">#</span><span class="bash"> 创建env环境</span>source  env_django/bin/activate # 载入py环境pip  install -i http://mirrors.aliyun.com/pypi/simple  --trusted-host mirrors.aliyun.com  -r install/requirements.txt<span class="hljs-meta">#</span><span class="bash">安装pip包(阿里源)</span>python manage.py makemigrations <span class="hljs-meta">#</span><span class="bash"> 为改动models创建迁移记录</span>python manage.py migrate <span class="hljs-meta">#</span><span class="bash"> 同步数据库</span>python manage.py  createsuperuser<span class="hljs-meta">#</span><span class="bash"> 建立后台管理员帐号</span>python manage.py runserver 0.0.0.0:8080<span class="hljs-meta">#</span><span class="bash">启动服务</span></code></pre><h3 id="效果预览"><a href="#效果预览" class="headerlink" title="效果预览"></a>效果预览</h3><p>首页:</p><p><img src="https://gitee.com/attacker/cmdb/raw/master/doc/index.jpg" alt="shouye"></p><p>后台：</p><p><img src="https://gitee.com/attacker/cmdb/raw/master/doc/admin1.png"></p><p><img src="https://gitee.com/attacker/cmdb/raw/master/doc/admin2.png"></p><p><strong>参考地址</strong>：<a href="https://gitee.com/attacker/cmdb">https://gitee.com/attacker/cmdb</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>cmdb</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Redis学习笔记</title>
    <link href="/2020/08/31/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <url>/2020/08/31/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h3 id="一、介绍"><a href="#一、介绍" class="headerlink" title="一、介绍"></a>一、介绍</h3><p><a href="http://irfen.me/tag/redis">redis</a>是一个开源的高性能键值对<a href="http://irfen.me/tag/%E6%95%B0%E6%8D%AE%E5%BA%93">数据库</a>。他通过提供多种键值数据类型来适应不同场景下的存储需求，并借助许多高层级的接口使其可以胜任如缓存、队列系统等不同的角色。</p><blockquote><p><code>nosql </code>非关系数据库   key=》value</p><p>redis  memcached   内存高速缓存数据库</p></blockquote><h3 id="二、可持久化（数据会同步到磁盘中）"><a href="#二、可持久化（数据会同步到磁盘中）" class="headerlink" title="二、可持久化（数据会同步到磁盘中）"></a>二、可持久化（数据会同步到磁盘中）</h3><ul><li><h4 id="RDB"><a href="#RDB" class="headerlink" title="RDB"></a>RDB</h4></li></ul><blockquote><p>RDB其实就是把数据以快照的形式保存在磁盘上。什么是快照呢，你可以理解成把当前时刻的数据拍成一张照片保存下来。</p><p>RDB持久化是指在指定的时间间隔内将内存中的数据集快照写入磁盘。也是默认的持久化方式，这种方式是就是将内存中数据以快照的方式写入到二进制文件中,默认的文件名为<code>dump.rdb</code>。</p></blockquote><ul><li><h4 id="AOF"><a href="#AOF" class="headerlink" title="AOF"></a>AOF</h4><blockquote><p>全量备份总是耗时的，有时候我们提供一种更加高效的方式AOF，工作机制很简单，redis会将每一个收到的写命令都通过<code>write</code>函数追加到文件中。通俗的理解就是日志记录。</p></blockquote></li></ul><h3 id="二、REDIS设置外网访问"><a href="#二、REDIS设置外网访问" class="headerlink" title="二、REDIS设置外网访问"></a>二、REDIS设置外网访问</h3><h4 id="1、两个相关配置"><a href="#1、两个相关配置" class="headerlink" title="1、两个相关配置"></a>1、两个相关配置</h4><blockquote><p>bind:指定可以来连接redis实例的ip<br>protected-mode:保护模式（除本机外，其他的都无法连接）<br><code>protected-mode</code>启用的条件，第一是没有使用bind，第二个是没有设置访问密码</p></blockquote><h4 id="2、设置外网访问"><a href="#2、设置外网访问" class="headerlink" title="2、设置外网访问"></a>2、设置外网访问</h4><ol><li>注视bind并且把<code>protected-mode</code>改为no</li><li>使用bind</li><li>设置密码</li></ol><h3 id="三、redis集群搭建"><a href="#三、redis集群搭建" class="headerlink" title="三、redis集群搭建"></a>三、redis集群搭建</h3><h4 id="1、集群类型"><a href="#1、集群类型" class="headerlink" title="1、集群类型"></a>1、集群类型</h4><blockquote><p>主从<br>哨兵<br>高可用</p></blockquote><h4 id="2、高可用集群部署"><a href="#2、高可用集群部署" class="headerlink" title="2、高可用集群部署"></a>2、<strong>高可用</strong>集群部署</h4><ul><li><h5 id="准备机器"><a href="#准备机器" class="headerlink" title="准备机器"></a>准备机器</h5></li></ul><blockquote><p>192.168.220.128<br>192.168.220.129<br>192.168.220.130</p></blockquote><ul><li><h5 id="部署规划"><a href="#部署规划" class="headerlink" title="部署规划"></a>部署规划</h5></li></ul><p>一台机器上可以运行一主一从redis实例，通过不同的配置文件实现。</p><blockquote><blockquote><p>192.168.220.128:<br>端口：7001（主）    7004（从）<br>192.168.220.129：<br>端口：7002（主）    7005（从）<br>192.168.220.130：<br>端口：7003（主）    7006（从）</p></blockquote></blockquote><ul><li><h5 id="每台机器准备安装redis"><a href="#每台机器准备安装redis" class="headerlink" title="每台机器准备安装redis"></a>每台机器准备安装redis</h5></li></ul><pre><code class="hljs shell">/opt/redisredis-5.0.8.tar.gz/opt/redistar -zxvf redis-5.0.8.tar.gz -C /home/rediscd /home/redis/redis-5.0.8make</code></pre><ul><li><h5 id="修改redis-conf"><a href="#修改redis-conf" class="headerlink" title="修改redis.conf"></a>修改redis.conf</h5></li></ul><p>1、将源配置复制到专门存放配置文件目录</p><p>​    <strong>192.168.220.128：</strong></p><pre><code class="hljs shell">mkdir /home/redis/redis_conf/7001mkdir /home/redis/redis_conf/7004cp /home/redis/redis-5.0.8/src/redis.conf /home/redis/redis_conf/7001cp /home/redis/redis-5.0.8/src/redis.conf /home/redis/redis_conf/7004</code></pre><p>2、修改</p><p>需要修改的配置</p><p><strong>192.168.220.128:7001</strong></p><p>创建集群工作目录，conf中需要配置</p><pre><code class="hljs shell">mkdir -p /home/redis/cluster/7001mkdir -p /home/redis/cluster/7004</code></pre><pre><code class="hljs shell">vim /home/redis/redis_conf/7001</code></pre><blockquote><p>daemonize yes 后台模式<br>port 7001 端口号<br>#bind 127.0.0.1 注释本地绑定<br>dir /home/redis/cluster/7001  集群工作目录（根据实际情况提前创建）<br>cluster-enable yes 开启集群模式<br>cluster-config-file nodes-7001.conf 集群配置文件信息<br>cluster-node-timeout 超时设置<br>appendonly  yes  持久化<br>protected-mode no 保护模式<br>requirepass redis 密码<br>masterauth redis 集群通信密码（需要和requirepass密码保持一致）</p></blockquote><pre><code class="hljs shell">vim /home/redis/redis_conf/7004</code></pre><blockquote><p>daemonize yes 后台模式<br>port 7004 端口号<br>#bind 127.0.0.1 注释本地绑定<br>dir /home/redis/cluster/7004  集群工作目录（根据实际情况提前创建）<br>cluster-enable yes 开启集群模式<br>cluster-config-file nodes-7004.conf 集群配置文件信息<br>cluster-node-timeout 超时设置<br>appendonly  yes  持久化<br>protected-mode no 保护模式<br>requirepass redis 密码<br>masterauth redis 集群通信密码（需要和requirepass密码保持一致）</p></blockquote><p>5）启动主从服务</p><pre><code class="hljs shell">cd /home/redis/redis-5.0.8/src./redis-server /home/redis/redis_conf/7001/redis.conf（启动指定配置好的7001的主配置文件）./redis-server /home/redis/redis_conf/7004/redis.conf（启动指定配置好的7004的从配置文件）</code></pre><p>检查服务</p><p> ps -ef | grep redis</p><pre><code class="hljs shell">root      15109      1  0 15:44 ?        00:00:23 ./redis-server *:7001 [cluster]root      15114      1  0 15:45 ?        00:00:24 ./redis-server *:7004 [cluster]root      15418   7101  0 17:38 pts/0    00:00:00 grep --color=auto redis</code></pre><p><strong>至此，192.168.220.128上面7001和1004主从redis已完成，其他两台配置一样，注意修改conf文件中port、dir 、cluster-config-file三项配置。</strong></p><h4 id="3、集群启动"><a href="#3、集群启动" class="headerlink" title="3、集群启动"></a>3、集群启动</h4><p>选择其中一台机器启动：</p><p>cd /home/redis/redis-5.0.8/src</p><pre><code class="hljs shell">./redis-cli -a redis --cluster create --cluster-replicas 1 192.168.220.128:7001 192.168.220.129:7002 192.168.220.130:7003 192.168.220.128:7004 192.168.220.129:7005 192.168.220.130:7006</code></pre><p>启动日志：</p><pre><code class="hljs shell"><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing <span class="hljs-built_in">hash</span> slots allocation on 6 nodes...</span>Master[0] -&gt; Slots 0 - 5460Master[1] -&gt; Slots 5461 - 10922Master[2] -&gt; Slots 10923 - 16383Adding replica 192.168.220.129:7005 to 192.168.220.128:7001Adding replica 192.168.220.130:7006 to 192.168.220.129:7002Adding replica 192.168.220.128:7004 to 192.168.220.130:7003M: 5751ccf39c172abe7ede54ceb152cabfcc054340 192.168.220.128:7001   slots:[0-5460] (5461 slots) masterM: 9024ab249744a894a76bb993a5749c8985ce834d 192.168.220.129:7002   slots:[5461-10922] (5462 slots) masterM: f02a28331b391b8483dc3e8b083e38a1bee82b13 192.168.220.130:7003   slots:[10923-16383] (5461 slots) masterS: 31e33cc31cc35697fd2abf88c185658939a35271 192.168.220.128:7004   replicates f02a28331b391b8483dc3e8b083e38a1bee82b13S: c81c6332f0ad579329ddda6528786abfe22c513d 192.168.220.129:7005   replicates 5751ccf39c172abe7ede54ceb152cabfcc054340S: 981b250d5695c8dc54342e8dc19200a39b7c4313 192.168.220.130:7006   replicates 9024ab249744a894a76bb993a5749c8985ce834dCan I set the above configuration? (type &#x27;yes&#x27; to accept): yes<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Nodes configuration updated</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Assign a different config epoch to each node</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Sending CLUSTER MEET messages to join the cluster</span>Waiting for the cluster to join.............<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Performing Cluster Check (using node 192.168.220.128:7001)</span>M: 5751ccf39c172abe7ede54ceb152cabfcc054340 192.168.220.128:7001   slots:[0-5460] (5461 slots) master   1 additional replica(s)M: f02a28331b391b8483dc3e8b083e38a1bee82b13 192.168.220.130:7003   slots:[10923-16383] (5461 slots) master   1 additional replica(s)S: 31e33cc31cc35697fd2abf88c185658939a35271 192.168.220.128:7004   slots: (0 slots) slave   replicates f02a28331b391b8483dc3e8b083e38a1bee82b13S: c81c6332f0ad579329ddda6528786abfe22c513d 192.168.220.129:7005   slots: (0 slots) slave   replicates 5751ccf39c172abe7ede54ceb152cabfcc054340S: 981b250d5695c8dc54342e8dc19200a39b7c4313 192.168.220.130:7006   slots: (0 slots) slave   replicates 9024ab249744a894a76bb993a5749c8985ce834dM: 9024ab249744a894a76bb993a5749c8985ce834d 192.168.220.129:7002   slots:[5461-10922] (5462 slots) master   1 additional replica(s)[OK] All nodes agree about slots configuration.<span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check <span class="hljs-keyword">for</span> open slots...</span><span class="hljs-meta">&gt;</span><span class="bash">&gt;&gt; Check slots coverage...</span>[OK] All 16384 slots covered.</code></pre><p>连接集群命令：<code>./redis-cli -a redis -c -h  ip  -p  port</code></p><pre><code class="hljs shell">cd /home/redis/redis-5.0.8/src./redis-cli -a redis -c -h 192.168.220.129 -p 7002</code></pre><pre><code class="hljs shell">192.168.220.129:7002&gt; keys *(empty list or set)192.168.220.129:7002&gt; set k1 v1<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [12706] located at 192.168.220.130:7003</span>OK192.168.220.130:7003&gt; set k2 v2<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [449] located at 192.168.220.128:7001</span>OK192.168.220.128:7001&gt; keys *1) &quot;k2&quot;192.168.220.128:7001&gt; set k3 v3OK192.168.220.128:7001&gt; set k4 v4<span class="hljs-meta">-&gt;</span><span class="bash"> Redirected to slot [8455] located at 192.168.220.129:7002</span>OK192.168.220.129:7002&gt; quit</code></pre><p><strong>参考文档：<a href="https://www.jianshu.com/p/be14306f5fd8">https://www.jianshu.com/p/be14306f5fd8</a></strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Redis</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Harbor搭建与使用</title>
    <link href="/2020/08/30/Harbor%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/"/>
    <url>/2020/08/30/Harbor%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/</url>
    
    <content type="html"><![CDATA[<h3 id="Harbor介绍"><a href="#Harbor介绍" class="headerlink" title="Harbor介绍"></a>Harbor介绍</h3><p>Habor是由VMWare公司开源的容器镜像仓库。事实上，Habor是在Docker Registry上进行了相应的企业级扩展，从而获得了更加广泛的应用，这些新的企业级特性包括：管理用户界面，基于角色的访问控制 ，AD/LDAP集成以及审计日志等，足以满足基本企业需求。</p><h3 id="Harbor安装部署"><a href="#Harbor安装部署" class="headerlink" title="Harbor安装部署"></a>Harbor安装部署</h3><h4 id="一、前期准备"><a href="#一、前期准备" class="headerlink" title="一、前期准备"></a>一、前期准备</h4><h5 id="1、安装docker"><a href="#1、安装docker" class="headerlink" title="1、安装docker"></a>1、安装docker</h5><pre><code class="hljs shell">wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repoyum -y install docker-ce-18.06.1.ce-3.el7systemctl enable docker &amp;&amp; systemctl start dockerdocker --version</code></pre><blockquote><p>Docker version 18.06.1-ce, build e68fc7a</p></blockquote><h5 id="2、下载harbor安装包"><a href="#2、下载harbor安装包" class="headerlink" title="2、下载harbor安装包"></a>2、下载harbor安装包</h5><p><code>Github地址</code>：<a href="https://github.com/goharbor/harbor/releases/tag/v1.8.5">https://github.com/goharbor/harbor/releases/tag/v1.8.5</a></p><pre><code class="hljs shell">tar -zxvf harbor-offline-installer-v1.8.5.tgz -C /homepwd/home/harborlscommon  docker-compose.yml  harbor.v1.8.5.tar.gz  harbor.yml  install.sh  LICENSE  prepare</code></pre><h5 id="3、修改harbor-yml"><a href="#3、修改harbor-yml" class="headerlink" title="3、修改harbor.yml"></a>3、修改harbor.yml</h5><p><code> vim harbor.yml</code></p><blockquote><p>hostname: 192.168.220.131（修改为ip或者域名）<br>port: 8090 （默认80，自行修改）<br>harbor_admin_password: Harbor12345（登录harbor密码，自行修改）<br>data_volume: /home/data/harbor_data（数据存放目录，自行修改）</p></blockquote><p><code>vim /etc/docker/daemon.json</code></p><pre><code class="hljs shell">&#123; &quot;insecure-registries&quot; : [&quot;192.168.220.131:8090&quot;]&#125;</code></pre><p>重启docker</p><pre><code class="hljs bash">systemctl daemon-reloadsystemctl restart docker</code></pre><h5 id="4、访问harbor"><a href="#4、访问harbor" class="headerlink" title="4、访问harbor"></a>4、访问harbor</h5><pre><code class="hljs shell">http://192.168.220.131:8090/</code></pre><p>新建项目：<code>nathaniel-e</code></p><p>推送镜像：</p><blockquote><p>docker login <a href="http://192.168.220.131:8090/">http://192.168.220.131:8090</a><br>admin<br>Harbor12345<br>docker tag SOURCE_IMAGE[:TAG] 192.168.220.131:8090/nathaniel-e/IMAGE[:TAG]<br>docker push 192.168.220.131:8090/nathaniel-e/IMAGE[:TAG]</p></blockquote><p><strong>参考文档：</strong></p><p><strong><a href="https://www.jianshu.com/p/4f9474081c8a">https://www.jianshu.com/p/4f9474081c8a</a></strong></p><p><strong><a href="https://www.meiwen.com.cn/subject/qwedoftx.html">https://www.meiwen.com.cn/subject/qwedoftx.html</a></strong></p>]]></content>
    
    
    
    <tags>
      
      <tag>Harbor</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Zookeeper入门看这篇就够了</title>
    <link href="/2020/08/30/Zookeeper%E5%85%A5%E9%97%A8%E7%9C%8B%E8%BF%99%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/"/>
    <url>/2020/08/30/Zookeeper%E5%85%A5%E9%97%A8%E7%9C%8B%E8%BF%99%E7%AF%87%E5%B0%B1%E5%A4%9F%E4%BA%86/</url>
    
    <content type="html"><![CDATA[<h2 id="zookeeper特点"><a href="#zookeeper特点" class="headerlink" title="zookeeper特点"></a>zookeeper特点</h2><blockquote><ul><li>zookeeper：一个领导者（leader），多个跟随着（follower）组成的集群。</li><li>集群中只要有半数以上的节点存活，zookeeper集群就能正常服务。</li><li>全局数据一致：每个server保存相同的数据副本，client无论连接到哪一个server，数据都是一致的。</li><li>更新请求顺序进行，来自同一个client的更新请求按其发送顺序依次执行。</li><li>数据更新原子性，一次数据更新要么成功，要么失败。</li><li>实时性，在一定时间范围内，client连接到最新数据。</li></ul></blockquote><h2 id="zookeeper数据结构"><a href="#zookeeper数据结构" class="headerlink" title="zookeeper数据结构"></a>zookeeper数据结构</h2><blockquote><p>zookeeper数据模型和unix文件系统类似，每一个节点叫做一个ZNode。每一个ZNode默认能够存储1MB的数据，每个ZNode都可以通过其路径唯一标识。</p></blockquote><h2 id="zookeeper应用场景"><a href="#zookeeper应用场景" class="headerlink" title="zookeeper应用场景"></a>zookeeper应用场景</h2><ul><li><p>统一命名服务</p><p>分布式环境下，经常需要对应用服务统一命名，便于识别。</p><pre><code>`IP不容易记住，域名可以`</code></pre></li><li><p>统一配置管理</p><p>分布式环境下，配置文件同步。</p><ol><li>集群下，所有节点的配置信息是一致的，如<code>Kafka</code>集群</li><li>对配置文件修改后，快速同步到各个节点上</li></ol><p>配置管理可交由zookeeper实现。</p><ol><li>可将配置信息写入zookeeper上的一个<code>ZNode</code></li><li>各个客户端服务器监听这个<code>ZNode</code></li><li>一旦ZNode中的数据被修改，zookeeper将通知各个<code>客户端服务器</code></li></ol></li><li><p>统一集群管理</p><p>分布式环境下，实时掌握每个节点的状态。</p><ol><li>可根据节点实时状态做一些调整    </li></ol><p>zookeeper可以实现实时监控节点状态变化。</p><ol><li>可将节点信息写入zookeeper上的<code>ZNode</code></li><li>监听这个<code>NZode</code>可获取它的实时状态变化</li></ol></li><li><p>服务器动态上下线</p><p>客户端能实时洞察到服务器上下线的变化。</p></li><li><p>软负载均衡</p><p>在zookeeper中记录每台服务器的访问数，让访问数最少的服务器去处理最新的客户端请求。</p></li></ul><h2 id="zookeeper节点类型"><a href="#zookeeper节点类型" class="headerlink" title="zookeeper节点类型"></a>zookeeper节点类型</h2><blockquote><p><code>持久</code>：客户端和服务器端断开连接后，创建的节点不删除。</p><p><code>短暂</code>：客户端和服务器端断开连接后，创建的节点自己删除。</p></blockquote><h2 id="zookeeper集群部署"><a href="#zookeeper集群部署" class="headerlink" title="zookeeper集群部署"></a>zookeeper集群部署</h2><p><code>官网地址</code>：<a href="https://zookeeper.apache.org/">https://zookeeper.apache.org</a></p><ol><li><p>准备3台机器</p><blockquote><blockquote><p>vm001       192.168.220.128<br>vm002       192.168.220.129<br>vm003       192.168.220.130</p></blockquote></blockquote></li><li><p>每台机器安装jdk</p><pre><code class="hljs she">1）下载jdk包2）tar -zxvf jdk-8u191-linux-x64.tar.gz -C &#x2F;usr&#x2F;local3）添加环境变量：vim &#x2F;etc&#x2F;profileexport JAVA_HOME&#x3D;&#x2F;usr&#x2F;local&#x2F;jdk1.8.0_191export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin更新配置文件：source &#x2F;etc&#x2F;profile4）验证：   echo $JAVA_HOME    java -version</code></pre></li><li><p>下载apache-zookeeper-3.5.8-bin.tar.gz包</p><p><code>解压tar -zxvf apache-zookeeper-3.5.8-bin.tar.gz -C /home/zookeeper</code></p></li><li><p>每台机器修改配置</p><ul><li><p>首先将zoo_sample.cfg配置文件修名为<code>zoo.cfg</code></p></li><li><p>vim zoo.cfg</p><pre><code class="hljs shell">a、修改数据存储目录：dataDir=/opt/zkDatab、在配置末尾添加serverserver.1=192.168.220.128:2881:3881server.2=192.168.220.129:2881:3881server.3=192.168.220.130:2881:3881</code></pre></li><li><p>在数据目录/opt/zkData中创建myid文件</p></li></ul></li><li><p>启动服务</p><p><code>启动Server服务</code></p><pre><code class="hljs shell"><span class="hljs-meta">]#</span><span class="bash">./zkServer.sh start</span></code></pre><pre><code class="hljs shell">ZooKeeper JMX enabled by defaultUsing config: /home/zookeeper/apache-zookeeper-3.5.8-bin/bin/../conf/zoo.cfgStarting zookeeper ... STARTED</code></pre><p><code>查看状态</code></p><pre><code class="hljs shell"><span class="hljs-meta">]#</span><span class="bash">./zkServer.sh status</span></code></pre><pre><code class="hljs shell">ZooKeeper JMX enabled by defaultUsing config: /home/zookeeper/apache-zookeeper-3.5.8-bin/bin/../conf/zoo.cfgClient port found: 2181. Client address: localhost.Mode: leader</code></pre></li><li><p>注意事项</p><p>关闭防火墙，放通端口。</p></li></ol><pre><code class="hljs shell">firewall-cmd --zone=public --add-port=2181/tcp --permanentfirewall-cmd --zone=public --add-port=2881/tcp --permanentfirewall-cmd --zone=public --add-port=3881/tcp --permanentfirewall-cmd --reloadfirewall-cmd --zone=public --query-port=2181/tcpfirewall-cmd --zone=public --list-ports</code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Zookeeper</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Nginx笔记基于docker</title>
    <link href="/2020/08/30/Nginx%E5%AD%A6%E4%B9%A0%E5%9F%BA%E4%BA%8EDocker/"/>
    <url>/2020/08/30/Nginx%E5%AD%A6%E4%B9%A0%E5%9F%BA%E4%BA%8EDocker/</url>
    
    <content type="html"><![CDATA[<h3 id="Nginx介绍"><a href="#Nginx介绍" class="headerlink" title="Nginx介绍"></a>Nginx介绍</h3><h4 id="1-1引言"><a href="#1-1引言" class="headerlink" title="1.1引言"></a>1.1引言</h4><p><strong>为什么要学习Nginx</strong><br>问题1：客户端到底要将请求发送给哪台服务器<br>问题2：如果所有客户端的请求都发送给了服务器<br>问题3：客户端发送的请求可能是申请动态资源的，也有申请静态资源的</p><p><code>服务器搭建集群后</code></p><p><img src="https://img-blog.csdnimg.cn/20200804144611463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><p><code>在搭建集群后，使用Nginx做反向代理</code></p><p><img src="https://img-blog.csdnimg.cn/20200804144623426.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="{}}"></p><h4 id="1-2Nginx介绍"><a href="#1-2Nginx介绍" class="headerlink" title="1.2Nginx介绍"></a>1.2Nginx介绍</h4><p>Nginx是由俄罗斯人研发的，应对Rambler的网站并发，并且2004年发布的第一个版本。</p><p><code>Nginx的特点</code></p><ul><li>稳定性极强，7*24小时不间断运行(就是一直运行)</li><li>Nginx提供了非常丰富的配置实例</li><li>占用内存小，并发能力强(随便配置一下就是5w+,而tomcat的默认线程池是150)</li></ul><h3 id="Nginx的安装"><a href="#Nginx的安装" class="headerlink" title="Nginx的安装"></a>Nginx的安装</h3><h4 id="2-1安装Nginx"><a href="#2-1安装Nginx" class="headerlink" title="2.1安装Nginx"></a>2.1安装Nginx</h4><p>在/opt目录下创建docker_nginx目录</p><pre><code class="hljs shell">cd /optmkdir docker_nginx</code></pre><p>创建docker-compose.yml文件并编写下面的内容,保存退出</p><pre><code class="hljs shell">vim docker-compose.yml</code></pre><pre><code class="hljs json">version: &#x27;3.1&#x27;services:   nginx:    restart: always    image: daocloud.io/library/nginx:latest    container_name: nginx    ports:       - 80:80</code></pre><p>执行<code>docker-compose up -d</code></p><p>访问80端口，看到下图说明安装成功（ncthz.top是我阿里云服务器的域名，大家输入自己服务器的Ip就可以访问80端口了）</p><p><img src="https://img-blog.csdnimg.cn/20200804144702568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><h4 id="2-2Nginx的配置文件"><a href="#2-2Nginx的配置文件" class="headerlink" title="2.2Nginx的配置文件"></a>2.2Nginx的配置文件</h4><p>查看当前nginx的配置需要进入docker容器中</p><pre><code class="hljs shell">docker exec -it 容器id bash</code></pre><p>进入容器后</p><pre><code class="hljs shell">cd /etc/nginx/cat nginx.conf</code></pre><p><code>nginx.conf</code>文件内容如下</p><pre><code class="hljs json">user  nginx;worker_processes  1;error_log  /var/log/nginx/error.log warn;pid        /var/run/nginx.pid;# 以上同城为全局块# worker_processes的数值越大，Nginx的并发能力就越强# error_log代表Nginx错误日志存放的位置# pid是Nginx运行的一个标识events &#123;    worker_connections  1024;&#125;# events块# worker_connections的数值越大，Nginx的并发能力就越强http &#123;    include       /etc/nginx/mime.types;    default_type  application/octet-stream;    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    access_log  /var/log/nginx/access.log  main;    sendfile        on;    #tcp_nopush     on;    keepalive_timeout  65;    #gzip  on;    include /etc/nginx/conf.d/*.conf;&#125;# http块# include代表引入一个外部文件# include       /etc/nginx/mime.types;mime.types中存放着大量媒体类型#include /etc/nginx/conf.d/*.conf;引入了conf.d下以.conf为结尾的配置文件</code></pre><p>conf.d目录下只有一个<code>default.conf</code>文件，内容如下</p><pre><code class="hljs json">server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;    #charset koi8-r;    #access_log  /var/log/nginx/host.access.log  main;    location / &#123;        root   /usr/share/nginx/html;        index  index.html index.htm;    &#125;# location块# root:将接受到的请求根据/usr/share/nginx/html去查找静态资源# index:默认去上述的路径中找到index.html或index.htm    #error_page  404              /404.html;    # redirect server error pages to the static page /50x.html    #    error_page   500 502 503 504  /50x.html;    location = /50x.html &#123;        root   /usr/share/nginx/html;    &#125;    # proxy the PHP scripts to Apache listening on 127.0.0.1:80    #    #location ~ \.php$ &#123;    #    proxy_pass   http://127.0.0.1;    #&#125;    # pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000    #    #location ~ \.php$ &#123;    #    root           html;    #    fastcgi_pass   127.0.0.1:9000;    #    fastcgi_index  index.php;    #    fastcgi_param  SCRIPT_FILENAME  /scripts$fastcgi_script_name;    #    include        fastcgi_params;    #&#125;    # deny access to .htaccess files, if Apache&#x27;s document root    # concurs with nginx&#x27;s one    #    #location ~ /\.ht &#123;    #    deny  all;    #&#125;&#125;# server块# listen代表Nginx监听的端口号# server_name代表Nginx接受请求的IP</code></pre><h4 id="2-3修改docker-compose文件"><a href="#2-3修改docker-compose文件" class="headerlink" title="2.3修改docker-compose文件"></a>2.3修改<code>docker-compose</code>文件</h4><p>退出容器并关闭容器</p><pre><code class="hljs shell">exitdocker-compose down</code></pre><p>修改docker-compose.yml文件如下</p><pre><code class="hljs json">version: &#x27;3.1&#x27;services:   nginx:    restart: always    image: daocloud.io/library/nginx:latest    container_name: nginx    ports:       - 80:80    volumes:      - /opt/docker_nginx/conf.d/:/etc/nginx/conf.d</code></pre><p>重新构建容器</p><pre><code class="hljs shell">docker-compose bulid</code></pre><p>重新启动容器</p><pre><code class="hljs shell">docker-compose up -d</code></pre><p>这时我们再次访问80端口是访问不到的，因为我们映射了数据卷之后还没有编写server块中的内容</p><p>我们在<code>/opt/docker_nginx/conf.d</code>下新建<code>default.conf</code>，并插入如下内容</p><pre><code class="hljs json">server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;    location / &#123;        root   /usr/share/nginx/html;        index  index.html index.htm;    &#125;&#125;</code></pre><p>重启nginx</p><pre><code class="hljs shell">docker-compose restart</code></pre><p>这时我们再访问80端口，可以看到是访问成功的</p><h3 id="Nginx的反向代理"><a href="#Nginx的反向代理" class="headerlink" title="Nginx的反向代理"></a>Nginx的反向代理</h3><h4 id="3-1正向代理和反向代理介绍"><a href="#3-1正向代理和反向代理介绍" class="headerlink" title="3.1正向代理和反向代理介绍"></a>3.1正向代理和反向代理介绍</h4><p><code>正向代理</code>：<br>1.正向代理服务是由客户端设立的<br>2.客户端了解代理服务器和目标服务器都是谁<br>3.帮助咱们实现突破访问权限，提高访问的速度，对目标服务器隐藏客户端的ip地址</p><p><img src="https://img-blog.csdnimg.cn/20200804144856773.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><p><code>反向代理</code>：<br>1.反向代理服务器是配置在服务端的<br>2.客户端不知道访问的到底是哪一台服务器<br>3.达到负载均衡，并且可以隐藏服务器真正的ip地址</p><p><img src="https://img-blog.csdnimg.cn/20200804144918883.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><h4 id="3-2基于Nginx实现反向代理"><a href="#3-2基于Nginx实现反向代理" class="headerlink" title="3.2基于Nginx实现反向代理"></a>3.2基于Nginx实现反向代理</h4><blockquote><blockquote><p>准备一个目标服务器<br>启动tomcat服务器<br>编写nginx的配置文件(/opt/docker_nginx/conf.d/default.conf)，通过Nginx访问到tomcat服务器</p></blockquote></blockquote><p>准备tomcat服务器</p><pre><code class="hljs shell">docker run -d -p 8080:8080 --name tomcat  daocloud.io/library/tomcat:8.5.15-jre8</code></pre><p>或者已经下载了tomcat镜像</p><pre><code class="hljs shell">docker run -d -p 8080:8080 --name tomcat 镜像的标识</code></pre><p>添加数据卷</p><pre><code class="hljs shell">docker run -it -v /宿主机绝对目录:/容器内目录 镜像名</code></pre><p><code>default.conf</code>文件内容如下</p><pre><code class="hljs json">server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;    location / &#123;        proxy_pass http://ncthz.top:8080/;    &#125;&#125;</code></pre><p>重启nginx</p><pre><code class="hljs shell">docker-compose restart</code></pre><p>这时我们访问80端口可以看到8080端口tomcat的默认首页</p><h4 id="3-3关于Nginx的location路径映射"><a href="#3-3关于Nginx的location路径映射" class="headerlink" title="3.3关于Nginx的location路径映射"></a>3.3关于Nginx的location路径映射</h4><blockquote><p>优先级关系：<br>(location = ) &gt; (location /xxx/yyy/zzz) &gt; (location ^<del>) &gt; (location ~,</del>*) &gt; (location /起始路径) &gt; (location /)</p></blockquote><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash"> 1. = 匹配</span>location / &#123;<span class="hljs-meta">#</span><span class="bash">精准匹配，主机名后面不能带能和字符串</span><span class="hljs-meta">#</span><span class="bash">例如www.baidu.com不能是www.baidu.com/id=xxx</span>&#125;</code></pre><hr><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">2. 通用匹配</span>location /xxx &#123;<span class="hljs-meta">#</span><span class="bash">匹配所有以/xxx开头的路径</span><span class="hljs-meta">#</span><span class="bash">例如127.0.0.1:8080/xxxxxx可以为空，为空则和=匹配一样</span>&#125;</code></pre><hr><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">3. 正则匹配</span>location ~ /xxx &#123;<span class="hljs-meta">#</span><span class="bash">匹配所有以/xxx开头的路径</span>&#125;</code></pre><hr><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">4. 匹配开头路径</span>location ^~ /xxx/xx &#123;<span class="hljs-meta">#</span><span class="bash">匹配所有以/xxx/xx开头的路径</span>&#125;</code></pre><hr><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">5. 匹配结尾路径</span>location ~* \.(gif/jpg/png)$ &#123;<span class="hljs-meta">#</span><span class="bash">匹配以.gif、.jpg或者.png结尾的路径</span>&#125;</code></pre><p>修改<code>/opt/docker_nginx/conf.d/default.conf</code>如下</p><pre><code class="hljs json">server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;location /index &#123;        proxy_pass http://ncthz.top:8081/;#tomcat首页    &#125;location ^~ /CR/ &#123;        proxy_pass http://ncthz.top:8080/CR/;#毕设前台首页    &#125;    location / &#123;        proxy_pass http://ncthz.top:8080/CRAdmin/;#毕设后台首页    &#125;&#125;</code></pre><p>重启nginx</p><pre><code class="hljs shell">docker-compose restart</code></pre><p>访问ncthz.top/index可以进入tomcat首页<br>访问ncthz.top/CR/XXX可以进入毕设前台首页<br>访问ncthz.top或者ncthz.top:80可以进入毕设后台首页</p><h3 id="Nginx负载均衡"><a href="#Nginx负载均衡" class="headerlink" title="Nginx负载均衡"></a>Nginx负载均衡</h3><ul><li>Nginx为我们默认提供了三种负载均衡的策略：<br>轮询：<br>  将客户端发起的请求，平均分配给每一台服务器</li><li>权重：<br>  会将客户端的请求，根据服务器的权重值不同，分配不同的数量</li><li>ip_hash:<br>  基于发起请求的客户端的ip地址不同，他始终会将请求发送到指定的服务器上<br>  就是说如果这个客户端的请求的ip地址不变，那么处理请求的服务器将一直是同一个</li></ul><h4 id="4-1轮询"><a href="#4-1轮询" class="headerlink" title="4.1轮询"></a>4.1轮询</h4><p>想实现Nginx轮询负载均衡机制只需要修改配置文件如下</p><pre><code class="hljs json">upstream my_server&#123;    server ncthz.top:8080;    server ncthz.top:8081;&#125;server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;location / &#123;        proxy_pass http://my_server/;#tomcat首页    &#125;&#125;</code></pre><pre><code class="hljs json">upstream 名字&#123;    server ip:端口;    server 域名:端口;&#125;server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;location / &#123;        proxy_pass http://upstream的名字/;    &#125;&#125;</code></pre><p>重启nginx</p><pre><code class="hljs shell">docker-compose restart</code></pre><p>多次刷新ncthz.top页面，根据版本号我们可以发现我们进入的是不同的tomcat</p><p><img src="https://img-blog.csdnimg.cn/20200804145017833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><p><img src="https://img-blog.csdnimg.cn/2020080414503175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><h4 id="4-2权重"><a href="#4-2权重" class="headerlink" title="4.2权重"></a>4.2权重</h4><p>实现权重的方式：在配置文件中upstream块中加上<code>weight</code></p><pre><code class="hljs json">upstream my_server&#123;    server ncthz.top:8080 weight=10;    server ncthz.top:8081 weight=2;&#125;server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;location / &#123;        proxy_pass http://my_server/;#tomcat首页    &#125;&#125;</code></pre><h4 id="4-3ip-hash"><a href="#4-3ip-hash" class="headerlink" title="4.3ip_hash"></a>4.3ip_hash</h4><p>实现ip_hash方式：在配置文件upstream块中加上<code>ip_hash</code>;</p><pre><code class="hljs json">upstream my_server&#123;ip_hash;    server ncthz.top:8080 weight=10;    server ncthz.top:8081 weight=2;&#125;server &#123;    listen       80;    listen  [::]:80;    server_name  localhost;location / &#123;        proxy_pass http://my_server/;#tomcat首页    &#125;&#125;</code></pre><h3 id="Nginx动静分离"><a href="#Nginx动静分离" class="headerlink" title="Nginx动静分离"></a>Nginx动静分离</h3><blockquote><blockquote><p>Nginx的并发能力公式：<br> <code>worker_processes * worker_connections / 4|2 = Nginx最终的并发能力</code><br>动态资源需要/4，静态资源需要/2<br>Nginx通过动静分离来提升Nginx的并发能力，更快的给用户响应</p></blockquote></blockquote><h4 id="5-1动态资源代理"><a href="#5-1动态资源代理" class="headerlink" title="5.1动态资源代理"></a>5.1动态资源代理</h4><pre><code class="hljs shell"><span class="hljs-meta">#</span><span class="bash">配置如下</span>location / &#123;  proxy_pass 路径;&#125;</code></pre><h4 id="5-2静态资源代理"><a href="#5-2静态资源代理" class="headerlink" title="5.2静态资源代理"></a>5.2静态资源代理</h4><p>停掉nginx</p><pre><code class="hljs shell">docker-compose down</code></pre><p>修改docker-compose.yml添加静态资源数据卷<br>不同版本的静态资源位置可能不同，可以在2.2中查看默认的位置（location块中root后的路径）</p><p>启动nginx</p><pre><code class="hljs shell">docker-compose up -d</code></pre><pre><code class="hljs json">version: &#x27;3.1&#x27;services:   nginx:    restart: always    image: daocloud.io/library/nginx:latest    container_name: nginx    ports:       - 80:80    volumes:      - /opt/docker_nginx/conf.d/:/etc/nginx/conf.d      - /opt/docker_nginx/html/:/usr/share/nginx/html</code></pre><pre><code class="hljs json">在/opt/docker_nginx/html下新建一个index.html在index.html里面随便写点东西展示修改nginx的配置文件location / &#123;    root /usr/share/nginx/html;    index index.html;&#125;</code></pre><p>配置如下</p><pre><code class="hljs json">location / &#123;    root 静态资源路径;    index 默认访问路径下的什么资源;    autoindex on;#代表展示静态资源的全部内容，以列表的形式展开&#125;</code></pre><p>重启nginx</p><pre><code class="hljs shell">docker-compose restart</code></pre><p>访问ncthz.top如下</p><p><img src="https://img-blog.csdnimg.cn/20200804145109582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><h3 id="Nginx集群"><a href="#Nginx集群" class="headerlink" title="Nginx集群"></a>Nginx集群</h3><h4 id="6-1引言"><a href="#6-1引言" class="headerlink" title="6.1引言"></a>6.1引言</h4><blockquote><p>单点故障，避免nginx的宕机，导致整个程序的崩溃<br>准备多台Nginx<br>准备keepalived，监听nginx的健康情况<br>准备haproxy，提供一个虚拟的路径，统一的去接收用户的请求</p></blockquote><p><img src="https://img-blog.csdnimg.cn/20200804145126982.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L20wXzQ5NTU4ODUx,size_16,color_FFFFFF,t_70" alt="Image text"></p><h4 id="6-2搭建"><a href="#6-2搭建" class="headerlink" title="6.2搭建"></a>6.2搭建</h4><blockquote><p>先准备好以下文件放入/opt/docker_nginx_cluster目录中<br>然后启动容器    注意确保80、8081和8082端口未被占用(或者修改docker-compose.yml中的端口)<br>docker-compose up -d</p><p>然后我们访问8081端口可以看到master，访问8082端口可以看到slave<br>因为我们设置了81端口的master优先级未200比82端口的slave优先级100高，所以我们访问80端口看到的是master<br>现在我们模仿8081端口的nginx宕机了<br>docker stop 8081端口nginx容器的ID<br>这时我们再去访问80端口看到的就是slave了</p></blockquote><p><code>Dockerfile</code></p><pre><code class="hljs json">FROM nginx:1.13.5-alpineRUN apk update &amp;&amp; apk upgradeRUN apk add --no-cache bash curl ipvsadm iproute2 openrc keepalivedCOPY entrypoint.sh /entrypoint.shRUN chmod +x /entrypoint.shCMD [&quot;/entrypoint.sh&quot;]</code></pre><p><code>entrypoint.sh</code></p><pre><code class="hljs json">#!/bin/sh#/usr/sbin/keepalvined -n -l -D -f /etc/keepalived/keepalived.conf --dont-fork --log-console &amp;/usr/sbin/keepalvined -D -f /etc/keepalived/keepalived.confnginx -g &quot;daemon off;&quot;</code></pre><p><code>docker-compose.yml</code></p><pre><code class="hljs json">version: &quot;3.1&quot;services:  nginx_master:    build:      context: ./      dockerfile: ./Dockerfile    ports:      -8081:80    volumes:      - ./index-master.html:/usr/share/nnginx/html/index.html      - ./favicon.ico:/usr/share/nnginx/html/favicon.ico      - ./keepalived-master.conf:/etv/keepalived/keepalived.conf    networks:      static-network:        ipv4_address:172.20.128.2    cap_add:      - NET_ADMIN  nginx_slave:    build:      context: ./      dockerfile: ./Dockerfile    ports:      -8082:80    volumes:      - ./index-slave.html:/usr/share/nnginx/html/index.html      - ./favicon.ico:/usr/share/nnginx/html/favicon.ico      - ./keepalived-slave.conf:/etv/keepalived/keepalived.conf    networks:      static-network:        ipv4_address:172.20.128.3    cap_add:      - NET_ADMIN  proxy:    image:  haproxy:1.7-apline    ports:      - 80:6301    volumes:      - ./happroxy.cfg:/usr/local/etc/haproxy/haproxy.cfg    networks:      - static-networknetworks:  static-network:    ipam:      congig:        - subnet: 172.20.0.0/16</code></pre><p><code>keepalived-master.conf</code></p><pre><code class="hljs json">vrrp_script chk_nginx &#123;    script &quot;pidof nginx&quot;    interval 2&#125;vrrp_instance VI_1 &#123;    state MASTER    interface etch0#容器内部的网卡名称    virtual_router_id 33    priority 200#优先级    advert_int 1        autheentication &#123;    auth_type PASS    auth_pass letmein&#125;virtual_ipaddress &#123;        172.20.128.50#虚拟路径    &#125;track_script &#123;        chk_nginx    &#125;&#125;</code></pre><p><code>keepalived-slave.conf</code></p><pre><code class="hljs json">vrrp_script chk_nginx &#123;    script &quot;pidof nginx&quot;    interval 2&#125;vrrp_instance VI_1 &#123;    state BACKUP    interface etch0#容器内部的网卡名称    virtual_router_id 33    priority 100#优先级    advert_int 1        autheentication &#123;    auth_type PASS    auth_pass letmein&#125;virtual_ipaddress &#123;        172.20.128.50#虚拟路径    &#125;track_script &#123;        chk_nginx    &#125;&#125;</code></pre><p><code>haproxy.cfg</code></p><pre><code class="hljs json">globallog 127.0.0.1 local0maxconn 4096daemonnbproc 4defaultslog 127.0.0.1 local3mode httpoption dontlognulloption redispatchretries 2maxconn 2000balance roundrobintimeout connect 5000mstimeout client 5000mstimeout server 5000msfrontend mainbind *:6301default_backend webserverbackend webserveerserver nginx_master 127.20.127.50:80 check inter 2000 rise 2 fall 5</code></pre><p><code>index-master.html</code></p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>master！<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span></code></pre><p><code>index-slave.html</code></p><pre><code class="hljs html"><span class="hljs-tag">&lt;<span class="hljs-name">h1</span>&gt;</span>slave！<span class="hljs-tag">&lt;/<span class="hljs-name">h1</span>&gt;</span></code></pre>]]></content>
    
    
    
    <tags>
      
      <tag>Nginx</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>使用Git+hexo搭建个人博客</title>
    <link href="/2020/08/30/%E4%BD%BF%E7%94%A8Git-hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"/>
    <url>/2020/08/30/%E4%BD%BF%E7%94%A8Git-hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/</url>
    
    <content type="html"><![CDATA[<p>用流行的 Hexo 博客系统，利用 Github Pages，搭建个人博客，使用 Markdown 语法进行写作，发布博客后，借用 Markdown 美化工具，生成公众号、知乎、头条等平台的文章格式，发布到相应平台。</p><h3 id="概念说明"><a href="#概念说明" class="headerlink" title="概念说明"></a>概念说明</h3><ul><li><p><strong>Hexo</strong><br>Hexo 是流行的博客框架，能集成多种插件和主题，可以生成各种类型的博客，具有很好的生态圈，并且在不断升级优化中，不用担心年久失修。</p></li><li><p><strong>Github pages</strong><br>Github 是一个基于 Git 版本管理工具的代码托管平台，是全球最大的开源代码平台，pages 是 Github 给用户提供的一种建立静态网页的服务，创建好之后，只要将做好的静态页面上传到 Github 上，就能被全球的能联网的人访问了。</p></li><li><p><strong>Markdown</strong><br>Markdown 是一种简单标记方法，使用简单规则，就可以做出美观的页面，比使用 Html 标记等语言制作页面方便的多，熟悉了标记规则之后，创作过程可以完全忘记标记的存在，并且大多数编辑器，包括在线编辑器都支持 。Markdown 语法，使用 Markdown 语法写的文章，可以很方便的在不同平台上共享，而不用担心文章格式受到改变。</p></li><li><p><strong>Markdown 美化工具</strong><br>Markdown 之所以简单，是因为它只提炼了文章格式相关的标记，比如标题，列表，加粗，斜体，代码块等，而将样式部分完全交给了解释程序，也就是说相同的格式可以被美化为不同风格的文章样式，所以就有了 Markdown 美化工具，只要将 markdown 文本粘贴到编辑器，就能查看美化后的效果，并且还可以切换不同风格，直到自己满意。<br>简单说 Markdown 美化工具就是将 markdown 文本转换为 Html 文本。</p></li><li><p><strong>操作系统</strong><br>Hexo 是基于 Nodejs 的，所以支持 Windows、Linux 和 MacOS，这里只以 Windows 为例讲解。</p><h3 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h3><p>Hexo 是基于 Nodejs 的，也就是用 Nodejs 写的一个应用，另外 Hexo 相关组件和包都是放在 Github 上的，所以还需要一个 git 工具，以便和 Github 交互</p><h3 id="安装-nvm"><a href="#安装-nvm" class="headerlink" title="安装 nvm"></a>安装 nvm</h3><p>Nodejs 版本很多，软件包也很多，常常令初学者头疼，现在有了专门管理 Nodejs 环境的工具 nvm，通过 nvm 可以简单的安装不同版本的 Nodejs，并且可以在不同版本之间切换，从而解决 Nodejs 的版本混乱问题</p><ol><li>下载安装包<br>这里这里 <a href="https://github.com/coreybutler/nvm-windows/releases">https://github.com/coreybutler/nvm-windows/releases</a> 选择最新版本的安装包，下载 zip 文件，例如目前最新版是 1.1.7，下载地址是<br><a href="https://github.com/coreybutler/nvm-windows/releases/download/1.1.7/nvm-noinstall.zip">https://github.com/coreybutler/nvm-windows/releases/download/1.1.7/nvm-noinstall.zip</a></li><li>将下载的 zip 包解压到你的程序安装目录，例如 D:\Software\nvm</li><li>在刚才解压的文件夹，双击运行 install.cmd 文件，中间可能会提示需要使用管理员权限，选择是即可</li><li>由于 nodejs 软件目录国内访问较慢，有必要设置下软件源，即让 nvm 从指定的地方下载 nodejs，方法是在环境变量中加一个 <code>NVM_NODEJS_ORG_MIRROR=https://npm.taobao.org/mirrors/node</code><br>如果不清楚如何设置，可以参考这里<a href="https://jingyan.baidu.com/article/8ebacdf02d3c2949f65cd5d0.html">https://jingyan.baidu.com/article/8ebacdf02d3c2949f65cd5d0.html</a></li></ol><h3 id="安装-nodejs"><a href="#安装-nodejs" class="headerlink" title="安装 nodejs"></a>安装 nodejs</h3><p>有了 nvm 就能轻松地安装 nodejs 了</p><p>打开一个命令行窗口，输入 <code>nvm install 12.18.2</code></p><p><code>12.18.2</code> 为 nodejs 目前最新的稳定版本号，可以在这里参看最新的版本号 <a href="https://nodejs.org/en/download/">https://nodejs.org/en/download/</a></p><p>成功安装后，在命令行中输入 node –version 就可以看到版本号信息，如 v12.18.2，安装 nodejs 的同时，会安装好 npm，即 nodejs 的软件包管理器，用来管理 nodejs 的各种扩展软件包，如果了解 python 的话，和 pip 很相似</p><h3 id="安装-Hexo"><a href="#安装-Hexo" class="headerlink" title="安装 Hexo"></a>安装 Hexo</h3><p>Hexo 实际上是一个 nodejs 的软件包，可以通过 npm 来安装，在安装之前，最好换以下 npm 的软件源，以便国内访问更快些，在命令行中输入命令：</p><pre><code class="hljs shell">nvm node_mirror https://npm.taobao.org/mirrors/node/nvm npm_mirror https://npm.taobao.org/mirrors/npm/</code></pre><p>注意用的是 nvm 而不是 npm，这两个命令，会在 nvm 安装文件夹下创建一个 settings.txt 文件，写入镜像网址，当然不用上述命令，自己创建 settings.txt,输入镜像网址也是一样的，settings.txt 内容为：</p><pre><code class="hljs shell">node_mirror: https://npm.taobao.org/mirrors/node/npm_mirror: https://npm.taobao.org/mirrors/npm/</code></pre><ul><li>注意这里用的是 <strong>npm</strong> 而不是 nvm</li><li>参数 g 表示全局安装，即在任何目录下都可以使用</li></ul><p>安装完成后，输入 <code>hexo --version</code> 可以看到包括 hexo 版本在内一些环境信息</p><h3 id="安装-Git"><a href="#安装-Git" class="headerlink" title="安装 Git"></a>安装 Git</h3><p>Git 可以管理本地的软件版本，也能和 Github 打交道，而且 GIT 工具集成了 Linux 的命令环境，可以在 Git 命令行下使用 Linux 命令，而且语法高亮，比较方便</p><p>下载 Git windows 安装程序 <a href="https://git-scm.com/download/win">https://git-scm.com/download/win</a></p><p>完成后，双击运行安装程序</p><p>一路选择默认设计就行，因为配置项很多，如果不想一步步选，可以勾选安装窗口的 <code>only show new options</code> 勾选框</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wMS5wbmc?x-oss-process=image/format,png" alt="git 安装选项"></p><p>安装完成后，如果不使用 git 来管理代码（对于写作来说就是文章），可以不用关注 git 的相关用法，只要知道可以通过鼠标右键菜单，启动一个 git 命令窗口就行了</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wMi5wbmc?x-oss-process=image/format,png" alt="Git Bash Here "></p><blockquote><p>安装时不同的选择导致的快捷菜单可能不同</p></blockquote><h3 id="创建博客"><a href="#创建博客" class="headerlink" title="创建博客"></a>创建博客</h3><p>环境准备好后，就可以构建博客系统了.</p><h4 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h4><p>创建一个博客目录，例如：<code>D:\myblog</code>，到改目录下，鼠标右键，选择 <code>Git Bash Here</code> 菜单，会进入一个命令行窗口，和 Windows 的命令行窗口类似</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wMy5wbmc?x-oss-process=image/format,png" alt="git 命令窗口"></p><p>在这里输入命令 <code>hexo init</code>，回车，会下载博客程序，当提示 <code>INFO Start blogging with Hexo!</code> 时候，说明创建完成</p><p>接着输入 <code>hexo server</code> 或者 <code>hexo s</code> 启动 Hexo 博客程序了</p><p>在浏览器中输入 localhost:4000，就可以查看博客了</p><blockquote><p>有时候 localhost:4000 没有很长时间没有反应，可能是 4000 端口被其他程序占用了，这时输入 Ctrl + C 结束 Hexo 程序，换个端口启动就好了，例如 <code>hexo s -p 5000</code> , 参数 <code>-p</code> 表示指定端口，5000 为指定的端口</p></blockquote><h4 id="设置"><a href="#设置" class="headerlink" title="设置"></a>设置</h4><p>默认启动的博客是英文版的，另外默认的主题是 landscape，如果不喜欢可以更好</p><p>在博客根目录下，<code>_config.yml</code> 为配置文件，配置项很多，只需要设置很少部分就可以</p><p>站点设置 site，设置博客站点的基本信息</p><ul><li>title 站点名称</li><li>subtitle 副标题</li><li>description 站点描述</li><li>keywords 站点关键字，方便搜索引擎归类</li><li>author 作者名称</li><li>language 语言，简体中文为 <code>zh-CN</code></li><li>timezone 时区</li></ul><p>域名设置 URL，即为博客设置个网址</p><ul><li>url 网址，例如 <a href="http://blog.lixiaofei.cn/">http://blog.lixiaofei.cn</a></li><li>root 表示博客的根目录，如果设置为 /blog 那么网址会是 url 加上这个根路径</li><li>permalink 为固定网址，表示每篇文章的网址组成部分</li></ul><blockquote><p>如果自己的域名，需要购买，例如在万网上购买，之后需要实名认证，然后将域名解析到博客服务器地址上，服务器地址将在 Github pages 部分详细说明</p></blockquote><h4 id="主题"><a href="#主题" class="headerlink" title="主题"></a>主题</h4><p>Hexo 自带了 landscape 主题，可以跟换其他的</p><p>在 <a href="https://hexo.io/themes/index.html">https://hexo.io/themes/index.html</a> 这里预览可用主题</p><p>选择喜欢的主题，找到 github 地址，一般都在预览页中有提供，复制主题项目的 github 地址：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wNC5wbmc?x-oss-process=image/format,png" alt="img"></p><p>进入博客根目录，右键选择 <code>Git Bash Here</code>，打开 git 命令行，输入 <code>git clone [主题github 项目地址地址] theme/[主题英文名称]</code> 回车执行</p><p>执行完成后，博客目录下 theme 里 会多一个 主题英文名称的文件夹，这就是刚才下载的主题</p><p>打开 Hexo 配置文件 <code>_config.yml</code> 找到 <code>theme</code> 配置项，修改为 <code>主题英文名称</code></p><p>正常情况下刷浏览器的页面就可以看到新主题效果了，如果不行，执行下 <code>hexo server</code> 就可以了</p><p>每个主题有自己不同的配置项，配置文件是对应主题文件夹中的 <code>_config.yml</code>，相关配置可以参考该主题的帮助文档</p><p>这里建议选择使用者多，支持丰富的主题，相对而言文档全，bug 少</p><h4 id="Github"><a href="#Github" class="headerlink" title="Github"></a>Github</h4><p>Github 不仅是很多开源软件的仓库，还可以成为我们博客的服务器，最重要的是免费，国内有类似的产品 Gitee（码云）提供类似功能</p><h5 id="注册和创建项目"><a href="#注册和创建项目" class="headerlink" title="注册和创建项目"></a>注册和创建项目</h5><p>访问 <a href="https://github.com/">https://github.com</a>，选择 sign up 填写必要信息完成注册，注册完成后，点击 sign in 登录</p><p>登录后，点击页面右上角附近的 <code>加号</code>，在弹出菜单中选择 <code>New repository</code></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wNS5wbmc?x-oss-process=image/format,png" alt="img"></p><p>在打开的页面中，填写项目名称，可以随要填写，因为作为博客使用，选择公开仓库，并且勾选为项目创建 <code>Readme</code></p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wNi5wbmc?x-oss-process=image/format,png" alt="img"></p><blockquote><p>如果不勾线创建 Readme 的话，在创建 Pages 之前必须提交写内容项目中</p></blockquote><p>完成后，点击 <code>Create repository</code> 创建项目</p><p>成功后，会打开创建项目的页面</p><h5 id="设置-pages"><a href="#设置-pages" class="headerlink" title="设置 pages"></a>设置 pages</h5><p>在项目页面选择 <code>Settings</code> 选项卡</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wNy5wbmc?x-oss-process=image/format,png" alt="img"></p><p>然后一直往下找到 GitHub Pages 节段，在 Souce 栏，点击 None 下拉菜单，选择第一项 <code>master branch</code>:</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wOC5wbmc?x-oss-process=image/format,png" alt="img"></p><p>页面会刷新，刷新后，就创建好了 Pages，并且显示出这个项目 Pages 的网址:</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8wOS5wbmc?x-oss-process=image/format,png" alt="img"></p><p>其中 <code>Custom domain</code> 可以设置自己的域名</p><p>下面简单说下域名</p><h4 id="域名"><a href="#域名" class="headerlink" title="域名"></a>域名</h4><p>域名是网络中的一个地址，比较方便记忆，另外有意义的域名对网站有一定的宣传作用，比如 mi.com</p><p>一个域名可以有无数个二级域名，比如 域名为 abc.com，二级域名可以是 erp.abc.com，blog.abc.com，docs.abc.com 等等，每个二级域名可以是不同的地址，或者说可以对应不同的网站</p><p>域名可以通过域名代理商购买，国内一般可以在 万网 上购买（目前万网并入阿里云），国外一般可以在 GoDaddy 上购买。域名是有有效期的，到期之前可以续费，以延长用于时间</p><p>有了域名后，可以在域名供应商提供的管理工具中设置域名对应的地址，叫做解析，可以将域名解析到自己的 GitHub Pages 上，例如我的：</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8xMC5wbmc?x-oss-process=image/format,png" alt="img"></p><ul><li>记录类型有多种，CNAME 表示指向另一个网址</li><li>主记录可以设置二级域名</li><li>记录值，当记录类型为 CNAME 时，表示指向的一个网址，例如 自己的 GitHub Pages 地址</li></ul><p>这时就可以在 GitHub 的项目中 Settings 中的 GitHub Pages 节段中设置 <code>Custom domain</code> 了，填写自己的域名即可</p><p>设置完后，最多 24 小时后，访问自己的域名（主记录中的域名）就可以显示出自己的 GitHub Pages 了</p><h4 id="集成与部署"><a href="#集成与部署" class="headerlink" title="集成与部署"></a>集成与部署</h4><p>到此，所有的准备工作算是做完了，回顾一下，先搭建 Nodejs 环境，然后用 Npm 包管理工具安装 Hexo 应用，再由 Hexo 创建本地的博客系统，接下来注册 GitHub，并创建 Pages 项目，然后再设置域名，使通过域名可以访问到 Pages 项目</p><p>现在我们需要将本地的博客和 GitHub Pages 关联起来，以至于我们可以不用关注如何发布和运行博客系统，只需将精力集中在写作之上</p><p>首先将 Hexo 配置文件 _config.yml 中的 <code>url</code> 设置为自己的域名</p><p>然后，设置部署，在配置文件 _config.yml 中找到 <code>deploy</code>，配置为：</p><pre><code class="hljs yml"><span class="hljs-attr">deploy:</span>  <span class="hljs-attr">type:</span> <span class="hljs-string">git</span>  <span class="hljs-attr">repo:</span> <span class="hljs-string">https://github.com/&lt;username&gt;/&lt;project&gt;</span>  <span class="hljs-attr">branch:</span> <span class="hljs-string">master</span><span class="hljs-number">1234</span></code></pre><ul><li>type 为部署类型，git 表示部署到 Git 远程仓库中</li><li>repo 为 GitHub 上项目的地址，注意这个地址不是 GitHub Pages 的网址，例如我的 项目地址为：<a href="https://github.com/alisx/alisx.github.io.git">https://github.com/alisx/alisx.github.io.git</a>, Pages 网址为：alisx.github.io</li><li>branch 为需要部署的项目分支，一般设置为主分支就可以</li></ul><p>最后，需要安装一个 Hexo 插件，用来与 GitHub 交互，在博客所在目录下，启动 Git Bash 命令行工具执行：</p><pre><code class="hljs cmd">npm install hexo-deployer-git --save</code></pre><p>现在就可以使用 <code>hexo deploy</code> 命令将本地博客部署到 GitHub Pages 上了</p><blockquote><p>部署时可能会提示输入 GitHub 的登录用户名及密码，按照提示输入即可</p></blockquote><h3 id="写作"><a href="#写作" class="headerlink" title="写作"></a>写作</h3><p>现在一起就绪，如何来写作呢？</p><h5 id="新建"><a href="#新建" class="headerlink" title="新建"></a>新建</h5><p>在 Git 命令行中（只要是命令行都可以，不过 Git 命令行有高亮以及类 Linux 命令），进入博客根目录，输入 <code>hexo new draft my_first_article</code> 就可以在 <code>source</code> 文件夹下，创建一个名为 <code>my_first_article.md</code> 文件了，解释下这个命令</p><ul><li>new 表示创建一个文章</li><li>draft 意思为草稿，在这是其实是表示一个模板，即草稿模板，模板存放在 <code>scaffolds</code> 文件夹下，其中有个 <code>draft.md</code> 文件，另外还有 <code>post.md</code> 和 <code>page.md</code> 两个文件，在新建时使用模板，不仅会套用模板文件，还会将新建的文件放在 <code>source</code> 文件夹对应模板的文件夹中，例如上面的 <code>source\draft</code></li><li>最所以这么做，是因为处于 <code>draft</code> 状态的文章，不会被部署到博客上，只有在 <code>post</code> 和 <code>page</code> 状态的文章才会被部署</li></ul><h5 id="编辑"><a href="#编辑" class="headerlink" title="编辑"></a>编辑</h5><p>用 Markdown 编辑器，大概创建的草稿文件，例如 <code>my_first_article.md</code>，会发现文件中已经有了部分文字，这是来自于模板文件的，在最开头 <code>---</code> 之间的部分是文章的源信息，用来表示文章标题，作者，创建时间，分类，标签等信息，这些信息用于生成文章的 Html 页面</p><p>源信息之后，就是文章的正文部分，使用 Markdown 标记写作就可以了，如果不熟悉 Markdown 语法，请参考我之前写的 Markdown 教程: <a href="http://blog.lixiaofei.cn/2018-05-10-markdown/%EF%BC%8C%E8%AF%AD%E6%B3%95%E5%B9%B6%E4%B8%8D%E5%A4%8D%E6%9D%82%EF%BC%8C%E4%B8%BB%E8%A6%81%E6%98%AF%E9%9C%80%E8%A6%81%E5%A4%9A%E5%8A%A0%E7%BB%83%E4%B9%A0%EF%BC%8C%E5%BD%A2%E6%88%90%E8%82%8C%E8%82%89%E8%AE%B0%E5%BF%86">http://blog.lixiaofei.cn/2018-05-10-markdown/，语法并不复杂，主要是需要多加练习，形成肌肉记忆</a></p><h5 id="预览"><a href="#预览" class="headerlink" title="预览"></a>预览</h5><p>预览是必要的环节，很多问题都是在这里发现的，虽然很多 Markdown 编辑器提供预览功能，但是能知道其在自己博客上的展示效果还是很有必要的</p><p>因为刚写的文章还在草稿状态，即在 <code>draft</code> 文件夹下，所以在预览时多加一个参数</p><pre><code class="hljs cmd">hexo server --draft</code></pre><p>这个命令和之前启动本地博客程序一样，不过多了个参数 <code>--draft</code>，其作用是可以将 draft 中的文章展示出来，从而达到预览的效果</p><h5 id="发布"><a href="#发布" class="headerlink" title="发布"></a>发布</h5><p>文章写完了，检查没有问题，就可以发布了</p><p>首先需要将文章从 <code>draft</code> 状态修改为 <code>post</code> 状态，使用命令 <code>hexo publish my_first_article</code>，执行完后，可以将 <code>draft</code> 文件夹中的名称为 <code>my_first_article.md</code> 的文章移动到 <code>post</code> 文件夹，来完成发布，此时执行 <code>hexo server</code> 不用加 <code>--draft</code> 参数就可以在浏览器中查看了</p><p>然后，需要将文章转换为 html 格式，因为 GitHub Pages 上只能展示静态的网页，使用的命令是 <code>hexo generate</code>，其会将 <code>post</code> 文件夹中的修改过的 Markdown 文件转换为 html 格式的文件。</p><blockquote><p>这里有两个问题，1 怎么知道哪些文件被修改过了；2 转换后的文件放哪里；<br>Hexo 会对文章建立索引，记录下来文章的指纹，如果文章被修改过，文章的指纹会改变，从而知道是否被修改，然后会将转换后的文件结合博客的其他结构放在 <code>public</code> 文件夹下，也就是整个博客的静态网页文件</p></blockquote><p>生成博客的静态网页文件之后，就可以部署到 GitHub Pages 上了，命令很简单 <code>hexo deploy</code>，执行完后，用自己的域名或者 GitHub Pages 提供的网址就可以访问了，如果没有变化，很可能是因为网站缓存，稍等下刷新就能看到了</p><p>博客上的文章发布完成了，怎么发布到其他平台呢？</p><p>这里介绍个 Markdown 美化工具 <a href="https://mdnice.com/%EF%BC%8C%E5%8F%AF%E4%BB%A5%E5%B0%86">https://mdnice.com/，可以将</a> Markdown 文件转换为适合不同平台的 Html 格式</p><p><img src="https://imgconvert.csdnimg.cn/aHR0cDovL2Jsb2cubGl4aWFvZmVpLmNuL2ltYWdlcy8yMDIwLzA3L2hleG8tYmxvZy1ndWlkZS8xMS5wbmc?x-oss-process=image/format,png" alt="img"></p><p>可以复制 Markdown 格式文件内容到左侧栏，或者通过 <code>文件</code> 菜单导入 Markdown 文件</p><p>可以通过 <code>主题</code> 菜单选择不同分格的主题</p><p>最后在最右侧，选择 公众号、知乎、掘金三个平台，会将转换后的复制到剪切板，然后粘贴到对应平台的文章编辑器中</p><p>这里有个问题，如果文章中有图片如何处理，因为在博客中，图片地址都是相对地址，直接复制上去是显示不出来的，所以复制之前，将图片相对地址替换为实际网址，例如本文中的图片相对地址为：<code>/images/2020/07/hexo-blog-guide/01.png</code>，替换为：<code>http://blog.lixiaofei.cn/images/2020/07/hexo-blog-guide/01.png</code>，如果图片较多的话，可以用编辑器的批量替换功能</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>这篇文章比较长，感觉有些复杂，涉及的东西比较多比较杂，梳理一下就是 注册一个 GitHub，创建一个 Pages 项目，在本地安装一个 Hexo 博客系统，将本地博客发布到 Pages 上，虽然比较麻烦，但只做这么一次，搭建好了后，后面主要用来创作文章，会轻松很多。</p><p>事情总没有预想中梳理，如果过程中有问题，请给我留言，一起交流学习，总之，要相信没有解决不了的问题。</p></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>hexo</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
